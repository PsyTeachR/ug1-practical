[
["index.html", "Level 1 Data Skills Course Information", " Level 1 Data Skills 2019-2020 Course Information Hello and welcome to Level 1 Psychology at the University of Glasgow! This is the course book for Psych 1A and Psych 1B and will contain almost everything you need for the practical element of the course. We say almost because there’s some stuff that we need to host on Moodle for admin reasons, for example, resources related to the lectures and assignment submission links. However, for labs and coursework you should keep this book very close. In fact, it would be a very good idea to save a bookmark for this page to your phone and computer. For a more general overview of the course including contact details for key staff, University rules and regulations, and how your final course grade will be calculated, please refer to the Level 1 handbook. "],
["a-lab-1.html", "1 1A: Lab 1 1.1 Pre-class activities 1.2 In-class activities 1.3 Homework", " 1 1A: Lab 1 In this lab you will be introduced to the skills you will develop over the next year and beyond. This lab will introduce open science and why it is relevant to your development as a Psychologist. You will meet your groups and the staff that will be leading your labs and who you will get to know well over the two semesters. We can’t wait to get started! 1.1 Pre-class activities Before we get started learning about psychology, it’s useful to do a few admin jobs to make sure that you’re getting all the information you need and the best experience possible. 1.1.1 Activity 1: Make sure you know your University username and password and have your student ID card. This may seem a bit simple however, there’s nothing worse than turning up to a lab and not being able to access the computers. Make sure that you know your username and password and consider using a password manager so that you don’t need to remember it yourself. If something has gone wrong and your account isn’t working or if you’ve forgotten your password you can find the IT help support for students here. You will also need to bring your student card to the labs as we will scan it to take attendance. If you have lost your card you can find out how to get a replacement here. 1.1.2 Activity 2: Set-up your university e-mail on your phone Setting up your University e-mail on your phone is one of the most important things you can do to make sure you know what is going on. When we contact you, it will be through your University e-mail, and when you contact us you should use this e-mail so that we can verify your identity. Please do not use your personal gmail/hotmail etc. e-mail addresses. You need to make sure that you check your e-mail at least once a day. Failing to read the information we send you is not a valid excuse for missing something so if you have your email on your phone with push notifications enabled it makes it much more likely that you will do this. You can find instructions for how to set up your email on your phone here. You may find that you get a lot of e-mails as you progress through University. Please do not turn off your e-mail or turn off e-mails from Moodle because you will miss information. Instead, you can use folders and rules to help organise your inbox (e.g., you can set a rule that all emails from a mailing list go to a specific folder rather than your inbox to help keep things tidy). 1.1.3 Activity 3: Be social If you have a Twitter or Facebook account you may find it helpful to follow the School accounts. On Facebook we have a Level 1 group so that you can contact other students. On Twitter, you can follow the School of Psychology account, but many of your 1st year lecturers and GTAs are also active on Twitter and you may find it interesting to follow them as well - Emily Nordmann, Niamh Stack, Heather Cleland-Woods, Helena Paterson, Holly Scott, Steven McNair, Jude Stevenson, Carolina Kuepper-Tetzel. OK, let’s start talking about psychology. 1.1.4 The replication crisis There is an ongoing debate within Psychology regarding whether the discipline is experiencing a replication crisis. When we say replication we mean the extent to which consistent results are obtained when we repeat an experiment under comparable conditions. So, when we say replication crisis we mean that efforts to replicate past study findings often do not show the same results. For example, in 2015, a team of researchers examined social science experiments published in the academic journals Nature and Science between 2010 and 2015. Out of 21 experiments, 13 were successfully replicated. Why is this happening? When conducting research, you have to ask yourself a lot of questions: How many participants will I recruit? What variables am I interested in? What’s the best way to analyse my data? This is called researcher degrees of freedom. This means researchers often have a lot of flexibility and can approach data in a “have a look” manner. Crucially, researcher degrees of freedom can lead to researchers using questionable research practices such as: Failing to report all of a study’s dependent measures (e.g what you measure in the experiment) Collecting more data after peeking at your results in order to make your results turn out the way you wanted Failing to report all conditions in a study (e.g. Collecting data from three conditions - no/low/high anxiety - but only reporting two) Stopping data collection earlier than planned because you found results you wanted Only reporting studies that “worked” HARKing - Hypothesising After Results Are Known (e.g. claiming you predicted these results before collecting data) In some cases, researchers have even been known to falsify their data. A related concept is reproducibility. If research is reproducible it means someone else could take the information provided by the authors (e.g., data, method section, analysis code) and reproduce their results. This may sound like it should be easy enough to do, but as you’ll learn about in class and throughout the lectures, this is very difficult, partially because often researchers have not shared their data or code. Why should I care? Science does not occur in a vacuum. Work that is published and disseminated then goes on to influence public knowledge (e.g. through reporting in the media) and the decisions our politicians make every day. What can be done about it? During your time here at Glasgow, we will support you in developing skills and experience in open science practices. Open science practices are actions which aim to improve the transparency, accessibility, and reproducibility of scientific research. 1.1.5 Activity 4: Open science video Watch this video of Dr Simine Vazire talking about open science and why it is an important concept for us to understand in Psychology. 1.1.5.1 Additional resources around open science and reproducibility Is most published research wrong? Replicability and Reproducibility Debate with Professor Dorothy Bishop Why an Entire Field of Psychology Is in Trouble Beyond Cherry-picking by Amy Orben 1.2 In-class activities All of the slides that are presented in the Labs are available on the Psych 1A Moodle page. 1.2.1 Activity 5: Getting to know the data This semester we will use the same dataset each week to develop our skills and knowledge. The dataset is from Woodworth et al. (2018). You can download all the files here and you can find an explanation of the data here Web-based Positive Psychology Interventions: A Reexamination of Effectiveness. Take some time now to read through the study and familiarise yourself with the data you will be using over the coming weeks. 1.2.1.1 How to use zip files In this course we will often ask you to download a folder with multiple files and this will be stored on the server as a zip file. A zip file is a folder that contains files that have been compressed to make the file size smaller and enables you to download multiple files at once, however, before you use the files from a zip folder you first need to extract them. Click on the link to download the folder and select to save the zip file in your lab 1 folder. Navigate to the zip file and open it. You will see all the files it contains but don’t use these - click “Extract all” on the top ribbon. You will be asked to select a location to save the unzipped files. Normally the default location it suggests will be the same folder and so you can just click “Extract”. You can now delete the zip file and use the unzipped files. This is a really important step - if you use the compressed files your code may not work properly. Figure 1.1: Unzipping a folder Pause and test your knowledge! When you get the correct answer, the answer box will turn green. Sometimes this doesn’t work on Internet Explorer or Edge so be sure to use Chrome or Firefox. What does the AHI measure? (Hint, use a single word) What does the CES-D measure? (hint, use a single word) In groups, have a think about what a possible research question might look like. How would you use this data? Would you be interested in studying any other variables alongside the data that you have? 1.3 Homework For the homework this week, we will go over some basic programming concepts and terminology, common pitfalls, helpful hints, and where to get help. Those of you who have no programming experience should find this chapter particularly helpful, however, even if you’ve used R before there may be some helpful hints and tips so please make sure you read through this chapter before the lab. You do not need to formally submit this homework but we will check that you’ve done it in the next lab! This is a long chapter but we don’t expect you to memorise all the information that is contained in it and some sections of it will make not make sense until you start writing your own code in the lab - just make sure you know what help is available! 1.3.1 Why are you teaching me programming?? I signed up for psychology not computer science!! Research methods and statistics is a large part of doing a psychology degree. In fact, it’s so important for your understanding of psychology that it’s one of the core areas that the British Psychological Society require for you to graduate with an accredited degree. Unfortunately, many students don’t realise that this is a part of psychology until the first week of lectures and for some people it comes as a bit of a shock. But! There is no need to worry. With both statistics and programming we’re going to start from the very beginning and develop your skills slowly over the next three years. The skills that you learn from this course will allow you to read research papers, design, conduct and analyse both qualitative and quantitative studies, evaluate research in all its forms (even fake news), and make some pretty fancy visualisations. Even if you don’t plan on a career in research, the transferable skills that you develop from this course will make you an employable and versatile graduate. The reason that we’re going to teach you how to do statistics and manage data using R is because of the issues we’ve been discussing regarding replication and reproducibility. Writing code is essentially like writing a fool-proof recipe. You can give other people (or just you after a long holiday) your raw data and code and they’ll be able to see exactly what you did, step-by-step with nothing missing. R is just the tool we will use to teach you how to do reproducible science so that you won’t make the same mistakes we did and so that psychological research will keep on improving. Right. Let’s begin. 1.3.2 R and R Studio For this course, you need two different bits of software, R and RStudio. R is a programming language that you will write code in and R Studio is an Integrated Development Environment (IDE) which makes working with R easier. Think of it as knowing English and using a plain text editor like NotePad to write a book versus using a word processor like Microsoft Word. You could do it, but it wouldn’t look as good and it would be much harder without things like spell-checking and formatting. In a similar way, you can use R without R Studio but we wouldn’t recommend it. The key thing to remember is that although you will do all of your work using R Studio for this course, you are actually using two pieces of software which means that from time-to-time, both of them may have separate updates. All of the University of Glasgow computers have R and R Studio installed, however, we can only guarantee that the computers in the Level 5 and 6 Boyd Orr psychology labs have the right set-up. Additionally, both are freely available so you may wish to install them on your own machine. There is a useful guide to installing them both here that you can use but if you need help wig this you can attend one of the PAL sessions or practice sessions (check your e-mails for details of when these run). Most computers will install R without any problems, however, if your computer cannot run it (for example, if you have a notebook lapatop), there is a browser version of R and you can find instructions for how to use this in Appendix A 1.3.3 Getting to know R Studio R Studio has a console that you can try out code in (appearing as the bottom left window in Figure 1.2), there is a script editor (top left), a window showing functions and objects you have created in the “Environment” tab (top right window in the figure), and a window that shows plots, files packages, and help documentation (bottom right). Figure 1.2: RStudio interface You will learn more about how to use the features included in R Studio throughout this course, however, we highly recommend watching RStudio Essentials 1 from the R Studio team. The video lasts ~30 minutes and gives a tour of the main parts of R Studio. 1.3.4 Functions and arguments Functions in R execute specific tasks and normally take a number of arguments (if you’re into linguistics you might want to think as these as verbs that require a subject and an object). You can look up all the arguments that a function takes by using the help documentation by using the format ?function. Some arguments are required, and some are optional. Optional arguments will often use a default (normally specified in the help documentation) if you do not enter any value. As an example, let’s look at the help documentation for the function rnorm() which randomly generates a set of numbers with a normal distribution. 1.3.5 Activity 1 Open up R Studio and in the console, type the following code: ?rnorm The help documentation for rnorm() should appear in the bottom right help panel. In the usage section, we see that rnorm() takes the following form: rnorm(n, mean = 0, sd = 1) In the arguments section, there are explanations for each of the arguments. n is the number of observations we want to create, mean is the mean of the data points we will create and sd is the standard deviation of the set. In the details section it notes that if no values are entered for mean and sd it will use a default of 0 and 1 for these values. Because there is no default value for n it must be specified otherwise the code won’t run. Let’s try an example and just change the required argument n to ask R to produce 5 random numbers. 1.3.6 Activity 2 Copy and paste the following code into the console. set.seed(12042016) rnorm(n = 5) ## [1] -0.2896163 -0.6428964 0.5829221 -0.3286728 -0.5110101 These numbers have a mean of 0 and an SD of 1. Now we can change the additional arguments to produce a different set of numbers. rnorm(n = 5, mean = 10, sd = 2) ## [1] 13.320853 9.377956 10.235461 9.811793 13.019102 This time R has still produced 5 random numbers, but now this set of numbers has a mean of 10 and an sd of 2 as specified. Always remember to use the help documentation to help you understand what arguments a function requires. If you’re looking up examples of code online, you may often see code that starts with the function set.seed(). This function controls the random number generator - if you’re using any functions that generate numbers randomly (such as rnorm()), running set.seed() will ensure that you get the same result (in some cases this may not be what you want to do). We call set.seed() in this example because it means that you will get the same random numbers as this book. 1.3.7 Argument names In the above examples, we have written out the argument names in our code (e.g., n, mean, sd), however, this is not strictly necessary. The following two lines of code would both produce the same result (although each time you run rnorm() it will produce a slightly different set of numbers, because it’s random, but they would still have the same mean and SD): rnorm(n = 6, mean = 3, sd = 1) rnorm(6, 3, 1) Importantly, if you do not write out the argument names, R will use the default order of arguments, that is for rnorm it will assume that the first number you enter is n. the second number is mean and the third number is sd. If you write out the argument names then you can write the arguments in whatever order you like: rnorm(sd = 1, n = 6, mean = 3) When you are first learning R, you may find it useful to write out the argument names as it can help you remember and understand what each part of the function is doing. However, as your skills progress you may find it quicker to omit the argument names and you will also see examples of code online that do not use argument names so it is important to be able to understand which argument each bit of code is referring to (or look up the help documentation to check). In this course, we will always write out the argument names the first time we use each function, however, in subsequent uses they may be omitted. 1.3.8 Tab auto-complete One very useful feature of R Studio is the tab auto-complete for functions (see Figure 1.3). If you write the name of the function and then press the tab key, R Studio will show you the arguments that function takes along with a brief description. If you press enter on the argument name it will fill in the name for you, just like auto-complete on your phone. This is incredibly useful when you are first learning R and you should remember to use this feature frequently. Figure 1.3: Tab auto-complete 1.3.9 Base R and packages When you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as Base R and there is a useful cheat sheet that shows many Base R functions here. However, the power of R is that it is extendable and open source - put simply, if a function doesn’t exist or doesn’t work very well, anyone can create a new package that contains data and code to allow you to perform new tasks. You may find it useful to think of Base R as the default apps that come on your phone and packages as additional apps that you need to download separately. 1.3.10 Installing and loading packages The Boyd Orr psychology computers will already have all of the packages you need for this course so you only need to install packages if you are using your own machine. Please do not install any packages on the university machines. 1.3.11 Activity 3: Install the tidyverse In order to use a package, you must first install it. The following code installs the package tidyverse, a package we will use very frequently in this course. If you are working on your own computer, use the below code to install the tidyverse. Do not do this if you are working on a University machine. install.packages(&quot;tidyverse&quot;) You only need to install a package once, however, each time you start R you need to load the packages you want to use, in a similar way that you need to install an app on your phone once, but you need to open it every time you want to use it. To load packages we use the function library(). Typically you would start any analysis script by loading all of the packages you need, but we will come back to that in the labs. 1.3.12 Activity 4: Load the tidyverse Run the below code to load the tidyverse. You can do this regardless of whether you are using your own computer or a University machine. library(tidyverse) You will get what looks like an error message - it’s not. It’s just R telling you what it’s done. Now that we’ve loaded the tidyverse package we can use any of the functions it contains but remember, you need to run the library() function every time you start R. 1.3.13 Package updates In addition to updates to R and R Studio, the creators of packages also sometimes update their code. This can be to add functions to a package, or it can be to fix errors. One thing to avoid is unintentionally updating an installed package. When you run install.packages() it will always install the latest version of the package and it will overwrite any older versions you may have installed. Sometimes this isn’t a problem, however, sometimes you will find that the update means your code no longer works as the package has changed substantially. It is possible to revert back to an older version of a package but try to avoid this anyway. To avoid accidentally overwriting a package with a later version, you should never include install.packages() in your analysis scripts in case you, or someone else runs the code by mistake. Remember, the Boyd Orr psychology computers will already have all of the packages you need for this course so you only need to install packages if you are using your own machine. 1.3.14 Package conflicts There are thousands of different R packages with even more functions. Unfortunately, sometimes different packages have the same function names. For example, the packages dplyr and MASS both have a function named select(). If you load both of these packages, R will produce a warning telling you that there is a conflict. library(dplyr) library(MASS) ## Warning: package &#39;MASS&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select In this case, R is telling you that the function select() in the dplyr package is being hidden (or ‘masked’) by another function with the same name. If you were to try and use select(), R would use the function from the package that was loaded most recently - in this case it would use the function from MASS. If you want to specify which package you want to use for a particular function you can use code in the format package::function, for example: dplyr::select() MASS::select() 1.3.15 Objects A large part of your coding will involve creating and manipulating objects. Objects contain stuff. That stuff can be numbers, words, or the result of operations and analyses.You assign content to an object using &lt;-. 1.3.16 Activity 5: Create some objects Copy and paste the following code into the console, change the code so that it uses your own name and age and run it. You should see that name, age, today, new_year, and data appear in the environment pane. name &lt;- &quot;emily&quot; age &lt;- 15 + 18 today &lt;-Sys.Date() new_year &lt;- as.Date(&quot;2020-01-01&quot;) data &lt;- rnorm(n = 10, mean = 15, sd = 3) Figure 1.4: Objects in the environment Note that in these examples, name,age, and new_year would always contain the values emily, 33, and the date of New Year’s Day 2020, however, today will draw the date from the operating system and data will be a randomly generated set of data so the values of these objects will not be static. As a side note, if you ever have to teach programming and statistics, don’t use your age as an example because everytime you have to update your teaching materials you get a reminder of the fragility of existence and your advancing age. Importantly, objects can be involved in calculations and can interact with each other. For example: age + 10 new_year - today mean(data) ## [1] 43 ## Time difference of -98 days ## [1] 17.66644 Finally, you can store the result of these operations in a new object: decade &lt;- age + 10 You may find it helpful to read &lt;- as contains, e.g., name contains the text emily. You will constantly be creating objects throughout this course and you will learn more about them and how they behave as we go along, however, for now it is enough to understand that they are a way of saving values, that these values can be numbers, text, or the result of operations, and that they can be used in further operations to create new variables. You may also see objects referred to as ‘variables’. There is a difference between the two in programming terms, however, they are used synonymously very frequently. 1.3.17 Looking after the environment If you’ve been writing a lot of code you may find that the environment pane (or workspace) has become cluttered with many objects. This can make it difficult to figure out which object you need and therefore you run the risk of using the wrong data frame. If you’re working on a new dataset, or if you’ve tried lots of different code before getting the final version, it is good practice to remember to clear the environment to avoid using the wrong object. You can do this in several way. To remove individual objects, you can type rm(object_name) in the console. Try this now to remove one of the objects you created in the previous section. To clear all objects from the environment run rm(list = ls()) in the console. To clear all objects from the environment you can also click the broom icon in the environment pane. Figure 1.5: Clearing the workspace 1.3.18 Setting the working directory What this means is that we need to tell R where the files we need are located. Think of it just like when you have different subjects, and you have separate folders for each topic e.g. biology, history and so on. When working on R, it’s useful to have all the data sets and files you need in one folder. To set the working directory press session -&gt; set working directory -&gt; choose directory and then select the folder where the data sets we are working on are saved, and save this file in the same folder as well. In other words- make sure your data sets and scripts are all in the same folder. In the labs, we recommend that you create a folder for your Psychology labs with sub-folders for the tutorial and data skills sections on your M: drive. This is your personal area on the University network that is safe and secure so is much better than flashdrives or desktops. You can access your M drive by logging into any computer on the University network. Figure 1.6: Setting your working directory 1.3.19 Activity 6: Set the working directory Create a new folder for Psych 1A on your M:Drive and then two sub-folders called Tutorial and Data Skills. Following the above instructions, set your working directory to your Data Skills folder. 1.3.20 R sessions When you open up R and start writing code, loading packages, and creating objects, you’re doing so in a new session. In addition to clearing the workspace, it can sometimes be useful to start a new session. This will happen automatically each time you start R, however, if you find your code isn’t working and you can’t figure out why, it might be worth starting a new session. This will clear the environment and detach all loaded packages - think of it like restarting your phone. To do this, click ‘Session - Restart R’. Remember that you will then need to load the packages you need and your data again. Figure 1.7: The truth about programming 1.3.21 Help and additional resources Figure 1.8: The truth about programming Getting good at programming really means getting good trying stuff out, searching for help online, and finding examples of code to copy. If you are having difficulty with any of the exercises contained in this book then you can ask for help on Slack or Moodle, however, learning to problem-solve effectively is a key skill that you need to develop throughout this course. Use the help documentation. If you’re struggling to understand how a function works, remember the ?function command. If you get an error message, copy and paste it in to Google - it’s very likely someone else has had the same problem. In addition to these course materials there are a number of excellent resources for learning R: R Cookbook StackOverflow R for Data Science Search or use the #rstats hashtag on Twitter 1.3.22 Debugging tips A large part of coding is trying to figure why your code doesn’t work and this is true whether you are a novice or an expert. As you progress through this course you should keep a record of mistakes you make and how you fixed them. In each chapter we will provide a number of common mistakes to look out for but you will undoubtedly make (and fix!) new mistakes yourself. Have you loaded the correct packages for the functions you are trying to use? One very common mistake is to write the code to load the package, e.g., library(tidyverse) but then forget to run it. Have you made a typo? Remember data is not the same as DATA and t.test is not the same as t_test. Is there a package conflict? Have you tried specifying the package and function with package::function? Is it definitely an error? Not all red text in R means an error - sometimes it is just giving you a message with information. 1.3.23 Activity 7: Test yourself Question 1. Why should you never include the code install.packages() in your analysis scripts? You should use library() instead Packages are already part of Base R You (or someone else) may accidentally install a package update that stops your code working You already have the latest version of the package Explain This Answer Remember, when you run install.packages() it will always install the latest version of the package and it will overwrite any older versions of the package you may have installed. Question 2.What will the following code produce? rnorm(6, 50, 10) A dataset with 10 numbers that has a mean of 6 and an SD of 50 A dataset with 6 numbers that has a mean of 50 and an SD of 10 A dataset with 50 numbers that has a mean of 10 and an SD of 6 A dataset with 50 numbers that has a mean of 10 and an SD of 6 Explain This Answer The default form for rnorm() is rnorm(n, mean, sd). If you need help remembering what each argument of a function does, look up the help documentation by running ?rnorm Question 3. If you have two packages that have functions with the same name and you want to specify exactly which package to use, what code would you use? package::function function::package library(package) install.packages(package) Explain This Answer You should use the form package::function, for example dplyr::select. Remember that when you first load your packages R will warn you if any functions have the same name - remember to look out for this! Question 4. Which of the following is most likely to be an argument? 35 read_csv() &lt;- Question 5. An easy way to spot functions is to look for brackets numbers computers. Question 6. The job of &lt;- is to send the output from the function to a/an argument assignment object. "],
["a-lab-2.html", "2 1A: Lab 2 2.1 Pre-class activities 2.2 In-class activities 2.3 Homework instructions", " 2 1A: Lab 2 2.1 Pre-class activities There are eight activities in total for this pre-class, but don’t worry, they are broken down into very small steps! 2.1.1 Activity 1: Create the working directory If you want to load data into R, or save the output of what you’ve created (which you almost always will want to do), you first need to tell R where the working directory is. All this means is that we tell R where the files we need (such as raw data) are located and where we want to save any files you have created. Think of it just like when you have different subjects, and you have separate folders for each topic e.g. biology, history and so on. When working with R, it’s useful to have all the data sets and files you need in one folder. We recommend making a new folder called “Psych 1A” with sub-folders for the tutorial and data skills secctions of the labs and saving any slides, activities, data, scripts, and homework files in these folders. We suggest that you create these folders on the M: drive. This is your personal area on the University network that is safe and secure so it is much better than flashdrives or desktops. Figure 2.1: Folder structure Choose a location for your lab work and then create the necessary folders. Whatever you do, do NOT call the folder you store your R work in “R”. You don’t have to call it Data Skills, but if you call it simply R then R will have an existential crisis about saving into itself and you will find that you have problems reading and saving your files. 2.1.2 Activity 2: Set the working directory Once you have created your folders, open R Studio. To set the working directory click Session -&gt; Set Working Directory -&gt; Choose Directory and then select the Data Skills folder as your working directory. 2.1.3 R Markdown for lab work and homework assignments For the lab work and homework you will use a worksheet format called R Markdown (abbreviated as Rmd) which is a great way to create dynamic documents with embedded chunks of code. These documents are self-contained and fully reproducible (if you have the necessary data, you should be able to run someone else’s analyses with the click of a button) which makes it very easy to share. This is an important part of your open science training as one of the reasons we are using R Studio is that it enables us to share open and reproducible information. Using these worksheets enables you to keep a record of all the code you write during the labs, and when it comes time for the portfolio assignments, we can give you a task you can and then fill in the required code. For more information about R Markdown feel free to have a look at their main webpage sometime http://rmarkdown.rstudio.com. The key advantage of R Markdown is that it allows you to write code into a document, along with regular text, and then knit it using the package knitr to create your document as either a webpage (HTML), a PDF, or Word document (.docx). 2.1.4 Activity 3: Open and save a new R Markdown document To open a new R Markdown document click the ‘new item’ icon and then click ‘R Markdown’. You will be prompted to give it a title, call it “Lab 2 pre-class”. Also, change the author name to your GUID as this will be good practice for the homework. Keep the output format as HTML. Once you’ve opened a new document be sure to save it by clicking File -&gt; Save as. You should also name this file “Lab 2 pre-class”. If you’ve set the working directory correctly, you should now see this file appear in your file viewer pane. Figure 2.2: Opening a new R Markdown document 2.1.5 Activity 4: Create a new code chunk When you first open a new R Markdown document you will see a bunch of welcome text that looks like this: Figure 2.3: New R Markdown text Do the following steps: * Delete everything below line 7 * On line 8 type “About me” * Click Insert -&gt; R Your Markdown document should now look something like this: Figure 2.4: New R chunk What you have created is a code chunk. In R Markdown, anything written in the white space is regarded as normal text, and anything written in a grey code chunk is assumed to be code. This makes it easy to combine both text and code in one document. When you create a new code chunk you should notice that the grey box starts and ends with three back ticks ```. One common mistake is to accidentally delete these back ticks. Remember, code chunks are grey and text entry is white - if the colour of certain parts of your Markdown doesn’t look right, check that you haven’t deleted the back ticks. 2.1.6 Activity 5: Write some code Now we’re going to use the code examples you read about in Lab 1 to add some simple code to our R Markdown document. In your code chunk write the below code but replace the values of name/age/birthday with your own details). Note that text values and dates need to be contained in quotation marks but numerical values do not. Missing and/or unnecessary quotation marks are a common cause of code not working - remember this! name &lt;- &quot;Emily&quot; age &lt;- 34 today &lt;- Sys.Date() next_birthday &lt;- as.Date(&quot;2020-07-11&quot;) 2.1.7 Running code When you’re working in an R Markdown document, there are several ways to run your lines of code. First, you can highlight the code you want to run and then click Run -&gt; Run Selected Line(s), however this is very slow. Figure 2.5: Slow method of running code Alternatively, you can press the green “play” button at the top-right of the code chunk and this will run all lines of code in that chunk. Figure 2.6: Slightly better method of running code Even better though is to learn some of the keyboard shortcuts for R Studio. To run a single line of code, make sure that the cursor is in the line of code you want to run (it can be anywhere) and press ctrl + enter. If you want to run all of the code in the code chunk, press ctrl + shift + enter. Learn these shortcuts, they will make your life easier! 2.1.8 Activity 6: Run your code Run your code using one of the methods above. You should see the variables name, age, today, and next_birthday appear in the environment pane. 2.1.9 Activity 7: Inline code An incredibly useful feature of R Markdown is that R can insert values into your writing using inline code. If you’ve ever had to copy and paste a value or text from one file in to another, you’ll know how easy it can be to make mistakes. Inline code avoids this. It’s easier to show you what inline code does rather than to explain it so let’s have a go. First, copy and paste this text exactly (do not change anything) to the white space underneath your code chunk. My name is `r name` and I am `r age` years old. It is `r next_birthday - today` days until my birthday. 2.1.10 Activity 8: Knitting your file Nearly finished! As our final step we are going to “knit” our file. This simply means that we’re going to compile our code into a document that is more presentable. To do this click Knit -&gt; Knit to HMTL. R Markdown will create a new HTML document and it will automatically save this file in your working directory. As if by magic, that slightly odd bit of text you copied and pasted now appears as a normal sentence with the values pulled in from the objects you created. My name is Emily and I am 34 years old. It is 94 days until my birthday. We’re not going to use this function very often in the rest of the course but hopefully you can see just how useful this would be when writing up a report with lots of numbers! R Markdown is an incredibly powerful and flexible format - this book was written using it! If you want to push yourself with R, additional functions and features of R Markdown would be a good place to start. Before we finish, there are a few final things to note about knitting that will be useful for the homework: R Markdown will only knit if your code works - this is a good way of checking for the portfolio assignments whether you’ve written legal code! You can choose to knit to a Word document rather than HTML. This can be useful for e.g., sharing with others, however, it may lose some functionality and it probably won’t look as good so we’d recommend always knitting to HTML. You can choose to knit to PDF, however, this requires an LaTex installation and is quite complicated. If you don’t already know what LaTex is and how to use it, do not knit to PDF. If you do know how to use LaTex, you don’t need us to give you instructions! R will automatically open the knitted HTML file in the viewer, however, you can also navigate to the folder it is stored in and open the HTML file in your web browser (e.g., Chrome or Firefox). 2.1.11 Finished And you’re done! On your very first time using R you’ve not only written functioning code but you’ve written a reproducible output! You could send someone else your R Markdown document and they would be able to produce exactly the same HTML document as you, just by pressing knit. The key thing we want you to take away from this pre-class is that R isn’t scary. It might be very new to a lot of you, but we’re going to take you through it step-by-step. You’ll be amazed at how quickly you can start producing professional-looking data visualisations and analysis. 2.2 In-class activities 2.2.1 Activity 1: APA referencing Look at the following reference and answer the below questions: Smith, M. K., Wood, W. B., Adams, W. K., Wieman, C., Knight, J. K., Guild, N., &amp; Su, T. T. (2009). Why peer discussion improves student performance on in-class concept questions. Science, 323(5910), 122-124. https://doi.org/10.1126/science.1165919 How many authors are there? What is the name of the journal? How many pages is the article? According to APA style: The title of the paper should be In bold text In italics In normal text. When using a direct in-text citation you should use and &amp; to connect the names of the authors. When citing a paper with multiple authors, if there are more than two authors the first time you cite the paper in the essay you should Write out all of the names Use et al. Explain this answer Et al means and others. It’s a common academic phrase that’s used when referring to work with more than three authors. 2.2.2 Data skills Part of becoming a psychologist is asking questions and gathering data to enable you to answer these questions effectively. It is very important that you understand all aspects of the research process such as experimental design, ethics, data management and visualisation. In this class, you will continue to develop reproducible scripts. This means scripts that completely and transparently perform an analysis from start to finish in a way that yields the same result for different people using the same software on different computers. And transparency is a key value of science, as embodied in the “trust but verify” motto. When you do things reproducibly, others can understand and check your work. This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. As part of your skill development, it is important that you work with data so that you can become confident and competent in your management and analysis of data. In the labs, we will work with real data that has been shared by other researchers. 2.2.3 Getting data ready to work with Today in the lab you will learn how to load the packages required to work with our data. You’ll then load the data into R Studio before getting it organised into a sensible format that relates to our research question. If you can’t remember what packages are, go back and revise 1.3.9. 2.2.4 Activity 2: Set-up Before we begin working with the data we need to do some set-up. If you need help with any of these steps, you should refer to Chapter 2.1: You should have already download the data files as part of Activity 5 in Lab 1, however, here they are again if you need them: Psych 1A Data Files. Extract the files and then move them in to your Data Skills folder. Download the Lab 2 Markdown File, extract it, and move it to your Data Skills folder. Open R and ensure the environment is clear. Set the working directory to your Data Skills folder. Open the stub-2.2.Rmd file and ensure that the working directory is set to your Data Skills folder and that the two .csv data files are in your working directory (you should see them in the file pane). 2.2.5 Activity 3: Load in the package Today we need to use the tidyverse package. You will use this package in every single lab on this course as the functions it contains are those we use for data wrangling, descriptive statistics, and visualisation. To load the tidyverse type the following code into your code chunk (not the console) and then run it. library(tidyverse) 2.2.6 Open data For this lab we are going to be using the real dataset that you looked at in Lab 1. Click the below link if you want a refresher of what the dataset contains. Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schüz, B., 2018. Data from, ‘Web-based Positive Psychology Interventions: A Reexamination of Effectiveness’. Journal of Open Psychology Data, 6(1). 2.2.7 Activity 4: Read in data Now we can read in the data. To do this we will use the function read_csv() that allows us to read in .csv files. There are also functions that allow you to read in .xlsx files and other formats, however in this course we will only use .csv files. First, we will create an object called dat that contains the data in the ahi-cesd.csv file. Then, we will create an object called info that contains the data in the participant-info.csv. dat &lt;- read_csv (&quot;ahi-cesd.csv&quot;) pinfo &lt;- read_csv(&quot;participant-info.csv&quot;) There is also a function called read.csv(). Be very careful NOT to use this function instead of read_csv() as they have different ways of naming columns. For the home, unless your results match ours exactly you will not get the marks which means you need to be careful to use the right functions. 2.2.8 Activity 5: Check your data You should now see that the objects dat and pinfo have appeared in the environment pane. Whenever you read data into R you should always do an initial check to see that your data looks like you expected. There are several ways you can do this, try them all out to see how the results differ. In the environment pane, click on dat and pinfo. This will open the data to give you a spreadsheet-like view (although you can’t edit it like in Excel) In the environment pane, click the small blue play button to the left of dat and pinfo. This will show you the structure of the object information including the names of all the variables in that object and what type they are (also see str(pinfo)) Use summary(pinfo) Use head(pinfo) Just type the name of the object you want to view, e.g., dat. 2.2.9 Activity 6: Join the files together We have two files, dat and info but what we really want is a single file that has both the data and the demographic information about the participants. R makes this very easy by using the function inner_join(). Remember to use the help function ?inner_join if you want more information about how to use a function and to use tab auto-complete to help you write your code. The below code will create a new object all_dat that has the data from both dat and pinfo and it will use the columns id and intervention to match the participants’ data. Run this code and then view the new dataset using one of the methods from Activity 4. all_dat &lt;- inner_join(x = dat, # the first table you want to join y = pinfo, # the second table you want to join by = c(&quot;id&quot;, &quot;intervention&quot;)) # columns the two tables have in common 2.2.10 Activity 7: Pull out variables of interest Our final step is to pull our variables of interest. Very frequently, datasets will have more variables and data than you actually want to use and it can make life easier to create a new object with just the data you need. In this case, the file contains the responses to each individual question on both the AHI scale and the CESD scale as well as the total score (i.e., the sum of all the individual responses). For our analysis, all we care about is the total scores, as well as the demographic information about participants. To do this we use the select() function to create a new object named summarydata. summarydata &lt;- select(.data = all_dat, # name of the object to take data from ahiTotal, cesdTotal, sex, age, educ, income, occasion,elapsed.days) # all the columns you want to keep Run the above code and then run head(summarydata). If everything has gone to plan it should look something like this: ahiTotal cesdTotal sex age educ income occasion elapsed.days 32 50 1 46 4 3 5 182.03 34 49 1 37 3 2 2 14.19 34 47 1 37 3 2 3 33.03 35 41 1 19 2 1 0 0.00 36 36 1 40 5 2 5 202.10 37 35 1 49 4 1 0 0.00 2.2.11 Activity 8: Visualise the data As you’re going to learn about more over this course, data visualisation is extremely important. Visualisations can be used to give you more information about your dataset, but they can also be used to mislead. We’re going to look at how to write the code to produce simple visualisations in Lab 3 and Lab 4, for now, we want to focus on how to read and interpret different kinds of graphs. Please feel free to play around with the code and change TRUE to FALSE and adjust the values and labels and see what happens but do not worry about understanding the code. Just copy and paste it. Copy, paste and run the below code to produce a bar graph that shows the number of female and male participants in the dataset. ggplot(summarydata, aes(x = as.factor(sex), fill = as.factor(sex))) + geom_bar(show.legend = FALSE, alpha = .8) + scale_x_discrete(name = &quot;Sex&quot;) + scale_fill_viridis_d(option = &quot;E&quot;) + scale_y_continuous(name = &quot;Number of participants&quot;)+ theme_minimal() Are there more male or more female participants (you will need to check the codebook to find out what 1 and 2 mean to answer this)? More female participants More male participants Copy, paste, and run the below code to create violin-boxplots of happiness scores for each income group. ggplot(summarydata, aes(x = as.factor(income), y = ahiTotal, fill = as.factor(income))) + geom_violin(trim = FALSE, show.legend = FALSE, alpha = .4) + geom_boxplot(width = .2, show.legend = FALSE, alpha = .7)+ scale_x_discrete(name = &quot;Income&quot;, labels = c(&quot;Below Average&quot;, &quot;Average&quot;, &quot;Above Average&quot;)) + scale_y_continuous(name = &quot;Authentic Happiness Inventory Score&quot;)+ theme_minimal() + scale_fill_viridis_d() The violin (the wavy line) shows density. Basically, the fatter the wavy shape, the more data points there are at that point. It’s called a violin plot because it very often looks (kinda) like a violin. The boxplot is the box in the middle. The black line shows the median score in each group. The median is calculated by arranging the scores in order from the smallest to the largest and then selecting the middle score. The other lines on the boxplot show the interquartile range. There is a really good explanation of how to read a boxplot here. The black dots are outliers, i.e., extreme values. Which income group has the highest median happiness score? Below average Average Above average Which income group has the lowest median happiness score? Below average Average How many outliers does the Average income group have? Finally, try knitting the file to HTML. And that’s it, well done! Remember to save your Markdown in your Lab 2 folder and make a note of any mistakes you made and how you fixed them. 2.2.11.1 Finished! Well done! You have started on your journey to become a confident and competent member of the open scientific community! To show us how competent you are you should now complete the homework for this lab which follows the same instructions as this in-class activity but asks you to work with different variables. Always use the lab prep materials as well as what you do in class to help you complete the class assessments! 2.2.12 Debugging tips When you downloaded the files from Moodle did you save the file names exactly as they were originally? If you download the file more than once you will find your computer may automatically add a number to the end of the file name. data.csv is not the same as data(1).csv. Pay close attention to names! Have you used the exact same object names as we did in each activity? Remember, name is different to Name. In order to make sure you can follow along with this book, pay special attention to ensuring you use the same object names as we do. Have you used quotation marks where needed? Have you accidentally deleted any back ticks (```) from the beginning or end of code chunks? 2.2.13 Test yourself When loading in a .csv file, which function should you use? read_csv() read.csv() Explain this answer Remember, in this course we use read_csv() and it is important for the homework that you use this function otherwise you may find that the variable names are slightly different and you won’t get the marks The function inner_join() takes the arguments x, y, by. What does by do? Specifies the first table to join Specifies the second table to join Specifies the column to join by that both tables have in common What does the function select() do? Keeps only the observations you specify Keeps only the variables you specify Keeps only the objects you specify 2.3 Homework instructions Just like you did in the pre-class, we’re going to use R Markdown for the homework sheets. If you haven’t done the pre-class, please work through it before attempting the homework. There are just a couple of important rules we need you to follow to make sure this all runs smoothly. These worksheets need to you fill in your answers and not change any other information. For example, if we ask you to replace NULL with your answer, only write in the code you are giving as your answer and nothing else. To illustrate - Task 1 read in your data data &lt;- NULL The task above is to read in the data file we are using for this task - the correct answer is data &lt;- read_csv(data.csv). You would replace the NULL with: Solution to Task 1 data &lt;- read_csv(&quot;data.csv&quot;) This means that we can look for your code and if it is in the format we expect to see it in, we can give you the marks! If you decide to get all creative on us then we can’t give you the marks as ‘my_lab_Nov_2018.csv’ isn’t the filename we have given to you to use. So don’t change the file, variable or data frame names as we need these to be consistent. We will look for your answers within the boxes which start and end with ``` and have {r task name} in them e.g. ```{r tidyverse, messages=FALSE} library(tidyverse) ``` These are called code chunks and are the part of the worksheet that we can read and pick out your answers. If you change these in any way we can’t read your answer and therefore we can’t give you marks. You can see in the example above that the code chunk (the grey zone), starts and ends with these back ticks (usually found on top left corner of the keyboard). This code chunk has the ticks and text which makes it the part of the worksheet that will contain code. The {r tidyverse} part tells us which task it is (e.g., loading in tidyverse) and therefore what we should be looking for and what we can give marks for - loading in the package called tidyverse in the example above. If this changes then it won’t be read properly, so will impact on your grade. The easiest way to use our worksheets is to think of them as fill-in-the-blanks and keep the file names and names used in the worksheet the same. If you are unsure about anything then use the forums on Moodle and Slack to ask any questions and come along to the practice sessions. 2.3.1 Homework files You can download all the R homework files and Assessment Information you need from the Lab Homework section of the Psych 1A Moodle. . "],
["a-lab-3.html", "3 1A: Lab 3 3.1 Pre-class activities 3.2 In-class activities 3.3 Activity 8: Make R your own 3.4 Homework", " 3 1A: Lab 3 In lab 2, you were introduced to the R environment (e.g. setting your working directory and the difference between .R and .Rmd files). You also began working with messy data by having a go at loading in datasets using read_csv(), joined files together using inner_join(), and pulled out variables of interest using select(). In lab 3, we’ll be moving on to becoming familiar with the Wickham Six and the functionality of the R package, tidyverse! 3.1 Pre-class activities Data comes in lots of different formats. One of the most common formats is that of a two-dimensional table (the two dimensions being rows and columns). Usually, each row stands for a separate observation (e.g. a subject), and each column stands for a different variable (e.g. a response, category, or group). A key benefit of tabular data is that it allows you to store different types of data-numerical measurements, alphanumeric labels, categorical descriptors-all in one place. It may surprise you to learn that scientists actually spend far more time cleaning and preparing their data than they spend actually analysing it. This means completing tasks such as cleaning up bad values, changing the structure of tables, merging information stored in separate tables, reducing the data down to a subset of observations, and producing data summaries. Some have estimated that up to 80% of time spent on data analysis involves such data preparation tasks (Dasu &amp; Johnson, 2003)! Many people seem to operate under the assumption that the only option for data cleaning is the painstaking and time-consuming cutting and pasting of data within a spreadsheet program like Excel. We have witnessed students and colleagues waste days, weeks, and even months manually transforming their data in Excel, cutting, copying, and pasting data. Fixing up your data by hand is not only a terrible use of your time, but it is error-prone and not reproducible. Additionally, in this age where we can easily collect massive datasets online, you will not be able to organise, clean, and prepare these by hand. In short, you will not thrive as a psychologist if you do not learn some key data wrangling skills. Although every dataset presents unique challenges, there are some systematic principles you should follow that will make your analyses easier, less error-prone, more efficient, and more reproducible. In this lesson you will see how data science skills will allow you to efficiently get answers to nearly any question you might want to ask about your data. By learning how to properly make your computer do the hard and boring work for you, you can focus on the bigger issues. 3.1.1 Tidyverse Tidyverse (https://www.tidyverse.org/) is a collection of R packages created by world-famous data scientist Hadley Wickham. Tidyverse contains six core packages: dplyr, tidyr, readr, purrr, ggplot2, and tibble. Last week when you typed library(tidyverse) into R, you will have seen that it loads in all of these packages in one go. Within these six core packages, you should be able to find everything you need to wrangle and visualise your data. In this chapter, we are going to focus on the dplyr package, which contains six important functions: select() Include or exclude certain variables (columns) filter() Include or exclude certain observations (rows) mutate() Create new variables (columns) arrange() Change the order of observations (rows) group_by() Organize the observations into groups summarise() Derive aggregate variables for groups of observations These six functions are known as ’single table verbs’ because they only operate on one table at a time. Although the operations of these functions may seem very simplistic, it’s amazing what you can accomplish when you string them together: Hadley Wickham has claimed that 90% of data analysis can be reduced to the operations described by these six functions. Again, we don’t expect you to remember everything in this pre-class - the important thing is that you know where to come and look for help when you need to do particular tasks. Being good at coding really is just being good at knowing what to copy and paste. 3.1.2 The babynames database To demonstrate the power of the six dplyr verbs, we will use them to work with the babynames data from the babynames package. The babynames dataset has historical information about births of babies in the U.S. 3.1.3 Activity 1: Set-up Do the following. If you need help, consult the Lab 2 materials. Download the lab 3 pre-class Markdown, extract the file and then move it in to your Data Skills folder. Open R Studio and ensure the environment is clear. Open the stub-3.1.Rmd file and ensure that the working directory is set to your Data Skills folder and that the two .csv data files are in your working directory (you should see them in the file pane). If you are working on your own computer, install the package babynames. Remember, never install packages if you are working on a university computer. The university computers will already have this package installed. Type and run the code that loads the packages tidyverse and babynames using library() in the Activity 1 code chunk. library(tidyverse) library(babynames) 3.1.4 Activity 2: Look at the data The package babynames contains an object of the same name that contains all the data about babynames. View a preview of this dataset by typing babynames in to the console. You should see the following output: babynames ## # A tibble: 1,924,665 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 ## 7 1880 F Ida 1472 0.0151 ## 8 1880 F Alice 1414 0.0145 ## 9 1880 F Bertha 1320 0.0135 ## 10 1880 F Sarah 1288 0.0132 ## # ... with 1,924,655 more rows The first line tells us that the object we are looking at is in a tibble with information on five variables with over 1.9 million rows. Yes, this dataset contains 1.8 million observations. Interested in analyzing these data by hand? No thanks! A tibble is basically a table of data presenting a two dimensional array of your data. Each row in the table represents data about births for a given name and sex in a given year. The variables are: variable type description year double (numeric) year of birth sex character recorded sex of baby (F = female, M = male) name character forename given to baby n integer number of babies given that name prop double (numeric) proportion of all babies of that sex The first row of the table tells us that in the year 1880, there were 7065 baby girls born in the U.S. who were given the name Mary, and this accounted for about 7% of all baby girls. 3.1.5 Activity 3: Data visualisation Type the code below into a new code chunk and run it. We’re going to cover how to write visualisation code in Lab so still don’t worry about not understanding the plot code yet. The point is show you how much you can accomplish with very little code. The code creates a graph showing the popularity of four girl baby names - Alexandra, Beverly, Emily, and Kathleen - from 1880 to 2014. You should see Figure 3.1 appear, which shows the proportion of each name across different years - you can plug in different names if you like and see how the plot changes. dat &lt;- babynames %&gt;% filter(name %in% c(&quot;Emily&quot;,&quot;Kathleen&quot;,&quot;Alexandra&quot;,&quot;Beverly&quot;), sex==&quot;F&quot;) ggplot(data = dat,aes(x = year,y = prop, colour=name))+ geom_line() Figure 3.1: Proportion of four baby names from 1880 to 2014 3.1.6 Activity 4: Selecting variables of interest There are two numeric measurements of name popularity, prop (the proportion of all babies with each name) is probably more useful than n (total number of babies with that name), because it takes into account that different numbers of babies are born in different years. Just like in Lab 2, if we wanted to create a dataset that only includes certain variables, we can use the select() function from the dplyr package. Run the below code to only select the columns year, sex, name and prop. select(.data = babynames, # the object you want to select variables from year, sex, name, prop) # the variables you want to select ## # A tibble: 1,924,665 x 4 ## year sex name prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1880 F Mary 0.0724 ## 2 1880 F Anna 0.0267 ## 3 1880 F Emma 0.0205 ## 4 1880 F Elizabeth 0.0199 ## 5 1880 F Minnie 0.0179 ## 6 1880 F Margaret 0.0162 ## 7 1880 F Ida 0.0151 ## 8 1880 F Alice 0.0145 ## 9 1880 F Bertha 0.0135 ## 10 1880 F Sarah 0.0132 ## # ... with 1,924,655 more rows Alternatively, you can also tell R which variables you don’t want, in this case, rather than telling R to select year, sex, name and prop, we can simply tell it to drop the column n using the minus sign - before the variable name. select(.data = babynames, -n) ## # A tibble: 1,924,665 x 4 ## year sex name prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1880 F Mary 0.0724 ## 2 1880 F Anna 0.0267 ## 3 1880 F Emma 0.0205 ## 4 1880 F Elizabeth 0.0199 ## 5 1880 F Minnie 0.0179 ## 6 1880 F Margaret 0.0162 ## 7 1880 F Ida 0.0151 ## 8 1880 F Alice 0.0145 ## 9 1880 F Bertha 0.0135 ## 10 1880 F Sarah 0.0132 ## # ... with 1,924,655 more rows Note that select() does not change the original tibble, but makes a new tibble with the specified columns. If you don’t save this new tibble to an object, it won’t be saved. If you want to keep this new dataset, create a new object. When you run this code, you will see your new tibble appear in the environment pane. new_dat &lt;- select(.data = babynames, -n) 3.1.7 Activity 5: Arranging the data The function arrange() will sort the rows in the table according to the columns you supply. Try running the following code: arrange(.data = babynames, # the data you want to sort name) # the variable you want to sort by ## # A tibble: 1,924,665 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2007 M Aaban 5 0.00000226 ## 2 2009 M Aaban 6 0.00000283 ## 3 2010 M Aaban 9 0.00000439 ## 4 2011 M Aaban 11 0.00000542 ## 5 2012 M Aaban 11 0.00000543 ## 6 2013 M Aaban 14 0.00000694 ## 7 2014 M Aaban 16 0.00000783 ## 8 2015 M Aaban 15 0.00000736 ## 9 2016 M Aaban 9 0.00000446 ## 10 2017 M Aaban 11 0.0000056 ## # ... with 1,924,655 more rows The data are now sorted in ascending alphabetical order by name. The default is to sort in ascending order. If you want it descending, wrap the name of the variable in the desc() function. For instance, to sort by year in descending order, run the following code: arrange(babynames,desc(year)) ## # A tibble: 1,924,665 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2017 F Emma 19738 0.0105 ## 2 2017 F Olivia 18632 0.00994 ## 3 2017 F Ava 15902 0.00848 ## 4 2017 F Isabella 15100 0.00805 ## 5 2017 F Sophia 14831 0.00791 ## 6 2017 F Mia 13437 0.00717 ## 7 2017 F Charlotte 12893 0.00688 ## 8 2017 F Amelia 11800 0.00629 ## 9 2017 F Evelyn 10675 0.00569 ## 10 2017 F Abigail 10551 0.00563 ## # ... with 1,924,655 more rows You can also sort by more than one column. What do you think the following code will do? arrange(babynames, desc(year), desc(sex), desc(prop)) 3.1.8 Activity 6: Using filter to select observations We have previously used select() to select certain variables or columns, however, frequently you will also want to select only certain observations or rows, for example, only babies born after 1999, or only babies named “Mary”. You do this using the verb filter(). The filter() function is a bit more involved than the other verbs, and requires more detailed explanation, but this is because it is also extremely powerful. Here is an example of filter, can you guess what it will do? filter(.data = babynames, year &gt; 2000) The first part of the code tells the function to use the object babynames. The second argument, year &gt; 2000, is what is known as a Boolean expression: an expression whose evaluation results in a value of TRUE or FALSE. What filter() does is include any observations (rows) for which the expression evaluates to TRUE, and exclude any for which it evaluates to FALSE. So in effect, behind the scenes, filter() goes through the entire set of 1.8 million observations, row by row, checking the value of year for each row, keeping it if the value is greater than 2000, and rejecting it if it is less than 2000. To see how a boolean expression works, consider the code below: years &lt;- 1996:2005 years years &gt; 2000 ## [1] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE You can see that the expression years &gt; 2000 returns a logical vector (a vector of TRUE and FALSE values), where each element represents whether the expression is true or false for that element. For the first five elements (1996 to 2000) it is false, and for the last five elements (2001 to 2005) it is true. Here are the most commonly used Boolean expressions. Operator Name is TRUE if and only if A &lt; B less than A is less than B A &lt;= B less than or equal A is less than or equal to B A &gt; B greater than A is greater than B A &gt;= B greater than or equal A is greater than or equal to B A == B equivalence A exactly equals B A != B not equal A does not exactly equal B A %in% B in A is an element of vector B If you want only those observations for a specific name (e.g., Mary), you use the equivalence operator ==. Note that you use double equal signs, not a single equal sign. filter(babynames, name == &quot;Mary&quot;) ## # A tibble: 268 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 M Mary 27 0.000228 ## 3 1881 F Mary 6919 0.0700 ## 4 1881 M Mary 29 0.000268 ## 5 1882 F Mary 8148 0.0704 ## 6 1882 M Mary 30 0.000246 ## 7 1883 F Mary 8012 0.0667 ## 8 1883 M Mary 32 0.000284 ## 9 1884 F Mary 9217 0.0670 ## 10 1884 M Mary 36 0.000293 ## # ... with 258 more rows If you wanted all the names except Mary, you use the ‘not equals’ operator: filter(babynames, name!=&quot;Mary&quot;) ## # A tibble: 1,924,397 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Anna 2604 0.0267 ## 2 1880 F Emma 2003 0.0205 ## 3 1880 F Elizabeth 1939 0.0199 ## 4 1880 F Minnie 1746 0.0179 ## 5 1880 F Margaret 1578 0.0162 ## 6 1880 F Ida 1472 0.0151 ## 7 1880 F Alice 1414 0.0145 ## 8 1880 F Bertha 1320 0.0135 ## 9 1880 F Sarah 1288 0.0132 ## 10 1880 F Annie 1258 0.0129 ## # ... with 1,924,387 more rows And if you wanted names from a defined set - e.g., names of British queens - you can use %in%: filter(babynames, name %in% c(&quot;Mary&quot;,&quot;Elizabeth&quot;,&quot;Victoria&quot;)) ## # A tibble: 772 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Elizabeth 1939 0.0199 ## 3 1880 F Victoria 93 0.000953 ## 4 1880 M Mary 27 0.000228 ## 5 1880 M Elizabeth 9 0.0000760 ## 6 1881 F Mary 6919 0.0700 ## 7 1881 F Elizabeth 1852 0.0187 ## 8 1881 F Victoria 117 0.00118 ## 9 1881 M Mary 29 0.000268 ## 10 1882 F Mary 8148 0.0704 ## # ... with 762 more rows This gives you data for the names in the vector on the right hand side of %in%. You can always invert an expression to get its opposite. So, for instance, if you instead wanted to get rid of all Marys, Elizabeths, and Victorias you would use the following: filter(babynames, !(name %in% c(&quot;Mary&quot;,&quot;Elizabeth&quot;,&quot;Victoria&quot;))) ## # A tibble: 1,923,893 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Anna 2604 0.0267 ## 2 1880 F Emma 2003 0.0205 ## 3 1880 F Minnie 1746 0.0179 ## 4 1880 F Margaret 1578 0.0162 ## 5 1880 F Ida 1472 0.0151 ## 6 1880 F Alice 1414 0.0145 ## 7 1880 F Bertha 1320 0.0135 ## 8 1880 F Sarah 1288 0.0132 ## 9 1880 F Annie 1258 0.0129 ## 10 1880 F Clara 1226 0.0126 ## # ... with 1,923,883 more rows You can include as many expressions as you like as additional arguments to filter() and it will only pull out the rows for which all of the expressions for that row evaluate to TRUE. For instance, filter(babynames, year &gt; 2000, prop &gt; .01) will pull out only those observations beyond the year 2000 that represent greater than 1% of the names for a given sex; any observation where either expression is false will be excluded. This ability to string together criteria makes filter() a very powerful member of the Wickham Six. Remember that this section exists. It will contain a lot of the answers to problems you face when wrangling data! 3.1.9 Activity 7: Creating new variables Sometimes we need to create a new variable that doesn’t exist in our dataset. For instance, we might want to figure out what decade a particular year belongs to. To create new variables, we use the function mutate(). Note that if you want to save this new column, you need to save it to an object. Here, you are mutating a new column and attaching it to the new_dat object you created in Activity 4. new_dat &lt;- mutate(.data = babynames, # the tibble you want to add a colum to decade = floor(year/10) *10) # new column name = what you want it to contain new_dat ## # A tibble: 1,924,665 x 6 ## year sex name n prop decade ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 1880 ## 2 1880 F Anna 2604 0.0267 1880 ## 3 1880 F Emma 2003 0.0205 1880 ## 4 1880 F Elizabeth 1939 0.0199 1880 ## 5 1880 F Minnie 1746 0.0179 1880 ## 6 1880 F Margaret 1578 0.0162 1880 ## 7 1880 F Ida 1472 0.0151 1880 ## 8 1880 F Alice 1414 0.0145 1880 ## 9 1880 F Bertha 1320 0.0135 1880 ## 10 1880 F Sarah 1288 0.0132 1880 ## # ... with 1,924,655 more rows In this case, you are creating a new column decade which has the decade each year appears in. This is calculated using the command decade = floor(year/10)*10. 3.1.10 Activity 8: Grouping and summarising Most quantitative analyses will require you to summarise your data somehow, for example, by calculating the mean, median or a sum total of your data. You can perform all of these operations using the function summarise(). First, let’s use the object dat that just has the data for the four girls names, Alexandra, Beverly, Emily, and Kathleen. To start off, we’re simply going to calculate the total number of babies across all years that were given one of these four names. It’s useful to get in the habit of translating your code into full sentences to make it easier to figure out what’s happening. You can read the below code as “run the function summarise using the data in the object dat to create a new variable named total that is the result of adding up all the numbers in the column n”. summarise(.data = dat, # the data you want to use total = sum(n)) # result = operation ## # A tibble: 1 x 1 ## total ## &lt;int&gt; ## 1 2161374 summarise() becomes even more powerful when combined with the final dplyr function, group_by(). Quite often, you will want to produce your summary statistics broken down by groups, for examples, the scores of participants in different conditions, or the reading time for native and non-native speakers. There are two ways you can use group_by(). First, you can create a new, grouped object. group_dat &lt;- group_by(.data = dat, # the data you want to group name) # the variable you want to group by If you look at this object in the viewer, it won’t look any different to the original dat, however, the underlying structure has changed. Let’s run the above summarise code again, but now using the grouped data. summarise(.data = group_dat, total = sum(n)) ## # A tibble: 4 x 2 ## name total ## &lt;chr&gt; &lt;int&gt; ## 1 Alexandra 231364 ## 2 Beverly 376914 ## 3 Emily 841491 ## 4 Kathleen 711605 summarise() has performed exactly the same operation as before - adding up the total number in the column n - but this time it has done is separately for each group, which in this case was the variable name. You can request multiple summary calculations to be performed in the same function. For example, the following code calculates the mean and median number of babies given each name every year. summarise(group_dat, mean_year = mean(n), median_year = median(n)) ## # A tibble: 4 x 3 ## name mean_year median_year ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alexandra 1977. 192 ## 2 Beverly 3089. 710. ## 3 Emily 6098. 1392. ## 4 Kathleen 5157. 3098 You can also add multiple grouping variables. For example, the following code groups new_dat by sex and decade and then calculates the summary statistics to give us the mean and median number of male and female babies in each decade. group_new_dat &lt;- group_by(new_dat, sex, decade) summarise(group_new_dat, mean_year = mean(n), median_year = median(n)) ## # A tibble: 28 x 4 ## # Groups: sex [2] ## sex decade mean_year median_year ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F 1880 111. 13 ## 2 F 1890 128. 13 ## 3 F 1900 131. 12 ## 4 F 1910 187. 12 ## 5 F 1920 211. 12 ## 6 F 1930 214. 12 ## 7 F 1940 262. 12 ## 8 F 1950 288. 13 ## 9 F 1960 235. 12 ## 10 F 1970 147. 11 ## # ... with 18 more rows 3.1.11 Activity 9: Pipes The final activity for this chapter essentially repeats what we’ve already covered but in a slightly different way. In the previous activities, you created new objects with new variables or groupings and then you called summarise() on those new objects in separate lines of code. As a result, you had multiple objects in your environment pane and you need to make sure that you keep track of the different names. Instead, you can use pipes. Pipes are written as %&gt;%and they should be read as “and then”. Pipes allow you to string together ‘sentences’ of code into ‘paragraphs’ so that you don’t need to create intermediary objects. Again, it is easier to show than tell. The below code does exactly the same as all the code we wrote above but it only creates one object. pipe_summary &lt;- mutate(babynames, decade = floor(year/10) *10) %&gt;% filter(name %in% c(&quot;Emily&quot;,&quot;Kathleen&quot;,&quot;Alexandra&quot;,&quot;Beverly&quot;), sex==&quot;F&quot;) %&gt;% group_by(name, decade) %&gt;% summarise(mean_decade = mean(n)) The reason that this function is called a pipe is because it ‘pipes’ the data through to the next function. When you wrote the code previously, the first argument of each function was the dataset you wanted to work on. When you use pipes it will automatically take the data from the previous line of code so you don’t need to specify it again. When learning to code it can be a useful practice to read your code ‘out loud’ in full sentences to help you understand what it is doing. You can read the code above as “create a new variable called decade AND THEN only keep the names Emily, Kathleen, Alexandra and Beverly that belong to female babies AND THEN group the dataset by name and decade AND THEN calculate the mean number of babies with each name per decade.” Try doing this each time you write a new bit of code. Some people find pipes a bit tricky to understand from a conceptual point of view, however, it’s well worth learning to use them as when your code starts getting longer they are much more efficient and mean you have to write less code which is always a good thing! 3.1.11.1 Finished! That was a long pre-class but remember, you don’t need to memorise all of this code. You just need to know where to look for help. 3.2 In-class activities 3.2.1 Activity 1: Set-up Do the following. If you need help, consult the Lab 2 materials. Download the lab 3 in-class Markdown, extract the file and then move it in to your Data Skills folder. Open R Studio and ensure the environment is clear. Open the stub-3.2.Rmd file and ensure that the working directory is set to your Data Skills folder and that the two .csv data files are in your working directory (you should see them in the file pane). Type and run the below code to load the tidyverse package and to load in the data files. library(tidyverse) dat &lt;- read_csv(&#39;ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;participant-info.csv&#39;) all_dat &lt;- inner_join(dat, pinfo, by= c(&quot;id&quot;, &quot;intervention&quot;)) Now let’s start working with our tidyverse verb functions… 3.2.2 Activity 2: Select Select the columns all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days from the data and create a variable called summarydata. summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) Pause here and interpret the above code and output Discuss in pairs or groups how you would translate this code into English What columns have been removed from the data? 3.2.3 Activity 3: Arrange Arrange the data in the variable created above (summarydata) by ahiTotal with lowest score first. ahi_asc &lt;- arrange(summarydata, by = ahiTotal) How could you arrange this data in descending order (highest score first)? Solution arrange(summarydata, by = desc(ahiTotal)) What is the smallest ahiTotal score? What is the largest ahiTotal score? 3.2.4 Activity 4: Filter Filter the data ahi_asc by taking out those who are over 65 years of age. age_65max &lt;- filter(ahi_asc, age &lt; 65) What does filter() do? splits a column into multiple columns transforms existing columns takes multiple columns and collapses them together removes information that we are not interested in How many observations are left in age_65max after running filter()? 3.2.5 Activity 5: Summarise Then, use summarise to create a new variable data_median, which calculates the median ahiTotal score in this grouped data and assign it a table head called median_score. data_median &lt;- summarise(age_65max, median_score = median(ahiTotal)) What is the median score? Change the above code to give you the mean score. What is the mean score to 2 decimal places? Solution summarise(age_65max, mean_score = mean(ahiTotal)) 3.2.6 Activity 6: Group_by Use mutate to create a new column called Happiness_Category in age_65max which categorises participants based on whether they score above the median ahiTotal score for all participants. Then, group the data stored in age_65max by Happiness_Category, and store it in an object named happy_dat. Finally, use summarise to calculate the median cesdTotal score for participants who scored above and below the median ahiTotal score and save it in a new object named data_median_group. age_65max &lt;- mutate(age_65max, Happiness_Category = (ahiTotal &gt; 74)) happy_dat &lt;- group_by(age_65max, Happiness_Category) data_median_group &lt;- summarise(happy_dat, median_score = median(cesdTotal)) Pause here and interpret the above code and output What does group_by() do? provides summary statistics of an existing dataframe organises information in ascending or descending order transforms existing columns groups data frames based on a specific column so that all later operations are carried out on a group basis How would you change the code to group by education rather than Happiness_Category? Solution group_by(age_65max, educ) 3.2.7 Activity 7: Data visualisation Copy, paste and run the below code into a new code chunk to create a plot of depression scores grouped by income level using the age_65max data. ggplot(age_65max, aes(x = as.factor(income), y = cesdTotal, fill = as.factor(income))) + geom_violin(trim = FALSE, show.legend = FALSE, alpha = .6) + geom_boxplot(width = .2, show.legend = FALSE, alpha = .5) + scale_fill_viridis_d(option = &quot;D&quot;) + scale_x_discrete(name = &quot;Income Level&quot;, labels = c(&quot;Below Average&quot;, &quot;Average&quot;, &quot;Above Average&quot;)) + scale_y_continuous(name = &quot;Depression Score&quot;) Which income group has the highest median depression scores? Below Average Average Above Average Which group has the highest density of scores at any one point? Below Average Average Above Average Explain This Answer Density is represented by the curvy line around the boxplot that looks a little bit like a (drunk) violin. The fatter the violin, the more data points there are at any one point. This means that in the above plot, the Above Average group has the highest density because this has the widest violin, i.e., there are lots of people in the Above Average income group with a score of about 5. Is income group a between-subject or within-subject variable? Between-subjects Within-subjects Explain This Answer Between-subjects designs are where different participants are in different groups. Within-subject designs are when the same participants are in all groups. Income is an example of a between-subject variable because participants can only be in one grouping level of the independent variable 3.3 Activity 8: Make R your own Finally, you can customise how R Studio looks to make it feel more like your own personal version. Click Tools - Global Options - Apperance. You can change the default font, font size, and general apperance of R Studio, including using dark mode. Play around with the settings and see which one you prefer - you’re going to spend a lot of time with R, it might as well look nice! 3.3.0.1 Finished! Well done! As a final step, try knitting the file to HTML. Remember to save your Markdown in your Lab 3 folder and make a note of any mistakes you made and how you fixed them. 3.4 Homework You can download all the R homework files and Assessment Information you need from the Lab Homework section of the Psych 1A Moodle. You should also begin to write your essay. "],
["a-lab-4.html", "4 1A: Lab 4 4.1 Pre-class activities 4.2 In-class activities 4.3 Homework", " 4 1A: Lab 4 4.1 Pre-class activities 4.1.1 Activity 1: dplyr recap In Lab 3 we were introduced to the tidyverse package, dplyr, and its six important functions. As a recap, which function(s) would you use to approach each of the following problems? We have a dataset of 400 adults, but we want to remove anyone with an age of 50 years or more. To do this, we could use the filter() summarise() mutate() select() arrange() group_by() function. We are interested in overall summary statistics for our data, such as the overall average and total number of observations. To do this, we could use the arrange() mutate() filter() select() summarise() group_by() function. Our dataset has a column with the number of cats a person has, and a column with the number of dogs. We want to calculate a new column which contains the total number of pets each participant has. To do this, we could use the select() filter() summarise() mutate() arrange() group_by() function. We want to calculate the average for each participant in our dataset. To do this we could use the group_by() and summarise() arrange() and mutate() group_by() and arrange() filter() and select() functions. We want to order a dataframe of participants by the number of cats that they own, but want our new dataframe to only contain some of our columns. To do this we could use the group_by() and mutate() select() and summarise() arrange() and select() mutate() and filter() functions. 4.1.2 Data visualisation As Grolemund and Wickham tell us: Visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them. (http://r4ds.had.co.nz/introduction.html) Being able to visualise our variables, and relationships between our variables, is a very useful skill. Before we do any statistical analyses or present any summary statistics, we should visualising our data as it is: A quick and easy way to check our data make sense, and to identify any unusual trends. A way to honestly present the features of our data to anyone who reads our research. ggplot() builds plots by combining layers (see Figure 4.1)). If you’re used to making plots in Excel this might seem a bit odd at first, however, it means that you can customise each layer and R is capable of making very complex and beautiful figures (this website gives you a good sense of what’s possible). Figure 4.1: ggplot2 layers from Field et al. (2012) 4.1.3 Activity 2: Set-up We’re going to use the data from the Labs to explain how ggplot2 works so let’s do the set-up as usual. Open R Studio and ensure the environment is clear. Download the lab 4 pre-class Markdown file, extract the file and then move it in to your Data Skills folder. Open the stub-4.1.Rmd file and ensure that the working directory is set to your Data Skills folder and that the two .csv data files are in your working directory (you should see them in the file pane). Type and run the below code to load the tidyverse package and to load in the data files in to the Activity 2 code chunk. library(tidyverse) dat &lt;- read_csv(&#39;ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;participant-info.csv&#39;) all_dat &lt;- inner_join(dat, pinfo, by= c(&quot;id&quot;, &quot;intervention&quot;)) summarydata &lt;- select(.data = all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) 4.1.4 Activity 3: Factors Before we go any further we need to perform an additional step of data processing that we have glossed over up until this point. First, run the below code to look at the structure of the dataset: str(summarydata) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 992 obs. of 8 variables: ## $ ahiTotal : num 32 34 34 35 36 37 38 38 38 38 ... ## $ cesdTotal : num 50 49 47 41 36 35 50 55 47 39 ... ## $ sex : num 1 1 1 1 1 1 2 1 2 2 ... ## $ age : num 46 37 37 19 40 49 42 57 41 41 ... ## $ educ : num 4 3 3 2 5 4 4 4 4 4 ... ## $ income : num 3 2 2 1 2 1 1 2 1 1 ... ## $ occasion : num 5 2 3 0 5 0 2 2 2 4 ... ## $ elapsed.days: num 182 14.2 33 0 202.1 ... R assumes that all of the variables are numeric (represented by num) and this is going to be a problem because whilst sex, educ, and income are represented by numerical codes, they aren’t actually numbers, they’re categories, or factors. We need to tell R that these variables are factors and we can use mutate() to do this by overriding the original variable with the same data but classified as a factor. Type and run the below code to change the categories to factors. summarydata &lt;- summarydata %&gt;% mutate(sex = as.factor(sex), educ = as.factor(educ), income = as.factor(income)) You can read this code as “overwrite the data that is in the column sex with sex as a factor”. Remember this. It’s a really important step and if your graphs are looking weird this might be the reason. 4.1.5 Activity 4: Bar plot For our first example we will recreate the bar plot showing the number of male and female participants from Lab 2 by showing you how the layers of code build up (next semester we have data that includes non-binary participants). The first line (or layer) sets up the base of the graph: the data to use and the aesthetics (what will go on the x and y axis, how the plot will be grouped). aes() can take both an x and y argument, however, with a bar plot you are just asking R to count the number of data points in each group so you don’t need to specify this. ggplot(summarydata, aes(x = sex)) Figure 4.2: First ggplot layer sets the axes The next layer adds a geom or a shape, in this case we use geom_bar() as we want to draw a bar plot. ggplot(summarydata, aes(x = sex)) + geom_bar() Figure 1.1: Basic barplot Adding fill to the first layer will separate the data into each level of the grouping variable and give it a different colour. In this case, there is a different coloured bar for each level of sex. ggplot(summarydata, aes(x = sex, fill = sex)) + geom_bar() Figure 4.3: Barplot with colour fill() has also produced a plot legend to the right of the graph. When you have multiple grouping variables you need this to know which groups each bit of the plot is referring to, but in this case it is redundant because it doesn’t tell us anything that the axis labels don’t already. We can get rid of it by adding show.legend = FALSE to the geom_bar() code. ggplot(summarydata, aes(x = sex, fill = sex)) + geom_bar(show.legend = FALSE) Figure 4.4: Barplot without legend We might want to tidy up our plot to make it look a bit nicer. First we can edit the axis labels to be more informative. The most common functions you will use are: scale_x_continuous() for adjusting the x-axis for a continuous variable scale_y_continuous() for adjusting the y-axis for a continuous variable scale_x_discrete() for adjusting the x-axis for a discrete/categorical variable scale_y_discrete() for adjusting the y-axis for a discrete/categorical variable And in those functions the two most common arguments you will use are: name which controls the name of each axis labels which controls the names of the break points on the axis There are lots more ways you can customise your axes but we’ll stick with these for now. Copy, paste, and run the below code to change the axis labels and change the numeric sex codes into words. ggplot(summarydata, aes(x = sex, fill = sex)) + geom_bar(show.legend = FALSE) + scale_x_discrete(name = &quot;Participant Sex&quot;, labels = c(&quot;Female&quot;, &quot;Male&quot;)) + scale_y_continuous(name = &quot;Number of participants&quot;) Figure 4.5: Barplot with axis labels Second, you might want to adjust the colours and the visual style of the plot. ggplot2 comes with built in themes. Below, we’ll use theme_minimal() but try typing theme_ into a code chunk and try all the options that come up to see which one you like best. ggplot(summarydata, aes(x = sex, fill = sex)) + geom_bar(show.legend = FALSE) + scale_x_discrete(name = &quot;Participant Sex&quot;, labels = c(&quot;Female&quot;, &quot;Male&quot;)) + scale_y_continuous(name = &quot;Number of participants&quot;) + theme_minimal() Figure 4.6: Barplot with minimal theme There are various options to adjust the colours but a good way to be inclusive is to use a colour-blind friendly palette that can also be read if printed in black-and-white. To do this, we can add on the function scale_fill_viridis_d(). This function has 5 colour options, A, B, C, D, and E. I prefer E but you can play around with them and choose the one you prefer. ggplot(summarydata, aes(x = sex, fill = sex)) + geom_bar(show.legend = FALSE) + scale_x_discrete(name = &quot;Participant Sex&quot;, labels = c(&quot;Female&quot;, &quot;Male&quot;)) + scale_y_continuous(name = &quot;Number of participants&quot;) + theme_minimal() + scale_fill_viridis_d(option = &quot;E&quot;) Figure 4.7: Barplot with colour-blind friendly colour scheme Finally, you can also adjust the transparency of the bars by adding alpha to geom_bar(). Play around with the value and see what value you prefer. ggplot(summarydata, aes(x = sex, fill = sex)) + geom_bar(show.legend = FALSE, alpha = .8) + scale_x_discrete(name = &quot;Participant Sex&quot;, labels = c(&quot;Female&quot;, &quot;Male&quot;)) + scale_y_continuous(name = &quot;Number of participants&quot;) + theme_minimal() + scale_fill_viridis_d(option = &quot;E&quot;) Figure 4.8: Barplot with adjusted alpha In R terms, ggplot2 is a fairly old package. As a result, the use of pipes wasn’t included when it was originally written. As you can see in the code above, the layers of the code are separated by + rather than %&gt;%. In this case, + is doing essentially the same job as a pipe - be careful not to confuse them. 4.1.6 Activity 5: Violin-boxplot As our final activity we will also explain the code used to create the violin-boxplot from Lab 2, hopefully now you will be able to see how similar it is in structure to the bar chart code. In fact, there are only three differences: We have added a y argument to the first layer because we wanted to represent two variables, not just a count. geom_violin() has an additional argument trim. Try setting this to TRUE to see what happens. geom_boxpot() has an additional argument width. Try adjusting the value of this and see what happens. ggplot(summarydata, aes(x = income, y = ahiTotal, fill = income)) + geom_violin(trim = FALSE, show.legend = FALSE, alpha = .4) + geom_boxplot(width = .2, show.legend = FALSE, alpha = .7)+ scale_x_discrete(name = &quot;Income&quot;, labels = c(&quot;Below Average&quot;, &quot;Average&quot;, &quot;Above Average&quot;)) + scale_y_continuous(name = &quot;Authentic Happiness Inventory Score&quot;)+ theme_minimal() + scale_fill_viridis_d() Figure 4.9: Violin-boxplot 4.1.7 Activity 6: Layers part 2 The key thing to note about ggplot is the use of layers. Whilst we’ve built this up step-by-step, they are independent and you could remove any of them except for the first layer. Additionally, although they are independent, the order you put them in does matter. Try running the two code examples below and see what happens. ggplot(summarydata, aes(x = income, y = ahiTotal)) + geom_violin() + geom_boxplot() ggplot(summarydata, aes(x = income, y = ahiTotal)) + geom_boxplot() + geom_violin() 4.1.7.1 Finished! Well done! ggplot can be a bit difficult to get your head around at first, particularly if you’ve been used to making graphs a different way. But once it clicks, you’ll be able to make informative and professional visualisations with ease, which, amongst other things, will make your reports look FANCY. 4.2 In-class activities 4.2.1 Getting the data ready to work with Today in the lab we will be working with our data to generate a plot of two variables from the Woodworth et al. dataset. Before we get to generate our plot, we still need to work through the steps to get the data in the shape we need it to be in for our particular question. In particular we need to generate the object summarydata that just has the variable we need.You have done these steps before so go back to the relevant Lab and use that to guide you through. 4.2.2 Activity 1: Set-up Open R Studio and ensure the environment is clear. Download the lab 4 in-class Markdown file, extract the file and then move it in to your Data Skills folder. Open the stub-4.2.Rmd file and ensure that the working directory is set to your Data Skills folder and that the two .csv data files are in your working directory (you should see them in the file pane). Look through your previous work to find the code that loads the tidyverse, reads in the data files and creates an object called all_dat that joins the two objects dat and pinfo. 4.2.3 Activity 2: Select Select the columns all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days from the data and create an object named variable summarydata. 4.2.4 Activity 3: Arrange Arrange the data in the variable created above (summarydata) by ahiTotal with lowest score first and save it in an object named ahi_asc. 4.2.5 Activity 4:Filter Filter the data ahi_asc by only keeping those who are 65 years old or younger and save it in an object named age_65max. 4.2.6 Activity 5: Group and summarise First, calculate the overall median ahiTotal score for all participants (hint: use summarise()) and save it in an object called overall_median. Then, group the data stored in the variable age_65max by sex, and store it in data_sex. Then, use summarise to create a new variable sex_median, which calculates the median ahiTotal score for each sex in this grouped data and assign it a table head called median_score. (Hint: if you’re stuck, see this dplyr documentation). 4.2.7 Activity 6: Mutate Use mutate() to create a new column in data_sex called Happiness which categorises participants based on whether they score above or below the overall median ahiTotal score (i.e., the median score for all participants, not grouped by sex). Hint mutate(data, new_variable = old_variable &gt; median score 4.2.8 Activity 7: Scatterplots In order to visualise two continuous variables, we can use a scatterplot. Using the ggplot code you learned about in the pre-class activities, try and recreate the below plot. A few hints: Use the age_65max data. Put ahiTotal on the x-axis and cesdTotal on the y-axis. Rather than using geom_bar(), geom_violin(), or geom_boxplot(), for a scatteplot you need to use geom_point(). Rather than using scale_fill_viridis_d() to change the colour, add the argument colour = \"red\" to geom_point (except replace “red” with whatever colour you’d prefer). Remember to edit the axis names. Figure 4.10: Scatterplot of happiness and depression scores How would you describe the relationship between the two variables? As happiness scores increase, depression scores increase As happiness score increase, depression scores decrease As happiness scores decrease, depression scores decrease 4.2.8.1 Finished! Great job! You have now worked with the essential basics of good practice in data wrangling! In Psych 1B we will continue using these wrangling skills on new data and also data that you collect yourself. 4.2.9 Activity solutions 4.2.9.1 Activity 1 Solution library(tidyverse) dat &lt;- read_csv (&#39;ahi-cesd.csv&#39;) pinfo &lt;- read_csv(&#39;participant-info.csv&#39;) all_dat &lt;- inner_join(dat, pinfo, by=c(&quot;id&quot;, &quot;intervention&quot;) 4.2.9.2 Activity 2 Solution summarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, elapsed.days) 4.2.9.3 Activity 3 Solution ahi_asc &lt;- arrange(summarydata, by = ahiTotal) 4.2.9.4 Activity 4 Solution age_65max &lt;- filter(ahi_asc, age &lt;= 65) 4.2.9.5 Activity 5 Solution overall_median &lt;- summarise(age_65max, median = median(ahiTotal)) data_sex &lt;- group_by(age_65max, sex) data_median &lt;- summarise(data_sex, median_score = median(ahiTotal)) 4.2.9.6 Activity 6 Solution happy &lt;- mutate(data_sex, Happiness = (ahiTotal &gt; 73)) 4.2.9.7 Activity 7 Solution ggplot(age_65max, aes(x = ahiTotal , y = cesdTotal)) + geom_point(colour = &quot;red&quot;) + scale_x_continuous(name = &quot;Happiness Score&quot;) + scale_y_continuous(name = &quot;Depression Score&quot;) + theme_minimal() 4.3 Homework You can download all the R homework files and Assessment Information you need from the Lab Homework section of the Psych 1A Moodle. "],
["b-lab-1.html", "5 1B: Lab 1 5.1 Pre-class activities 5.2 In-class activities", " 5 1B: Lab 1 5.1 Pre-class activities 5.1.1 Welcome back! Welcome back to Psych 1B! This semester we’re going to build on the data skills you developed in the first semester by adding in a couple of new data wrangling functions, running probability simulations in preparation for statistics in level 2, and analysing your own data for the group project. It would be nice to always get data formatted in the way that you want it, but one of the challenges as a scientist is dealing with Other People’s Data. People often structure data in ways that is convenient for data entry, but not very convenient for data analysis, and so, much effort must be expended ’wrangling’ data into shape before you can do more interesting things with it. Additionally, performing analyses often requires pulling together data obtained from different sources: you have done this in semester 1 by combining the participant information with the depression and happiness data. In this semester, we are going to give you some tips on how to structure data, and introduce strategies for transforming and combining data from different sources. 5.1.2 Autism-quotient data For Psych 1B we’re going to use a different dataset for our exercises based upon data that was collected using SurveyMonkey but that has has simulated variables added for the purposes of these exercises (gender was missing, so we have added this in). For this research project, participants completed the short 10-item version of the Autism-Spectrum Quotient (AQ) (Baron-Cohen, Wheelwright, Skinner, Martin, &amp; Clubley, 2001), which is designed to measure autistic traits in adults. The items for the quetionnaire are shown below. Table 1: The ten items on the AQ-10. Q_No Question Q 1 I often notice small sounds when others do not. Q 2 I usually concentrate more on the whole picture, rather than small details. Q 3 I find it easy to do more than one thing at once. Q 4 If there is an interruption, I can switch back to what I was doing very quickly. Q 5 I find it easy to read between the lines when someone is talking to me. Q 6 I know how to tell if someone listening to me is getting bored. Q 7 When I’m reading a story, I find it difficult to work out the characters’ intentions. Q 8 I like to collect information about categories of things. Q 9 I find it easy to work out what someone is thinking or feeling just by looking at their face. Q 10 I find it difficult to work out people’s intentions. Responses to each item were measured on a four-point scale: Definitely Disagree, Slightly Disagree, Slightly Agree, Definitely Agree. One of the issues with conducting research using surveys is that if we don’t design them carefully, our data may be affected by response bias. One type of response bias is acquiescence bias, which is the finding that people have a tendancy to agree with all statements. To try and minimise the impact of this, many questionnaires will reverse-code some of the questions so that a positive response means agreeing with one question but disagreeing with another. Read through the questions. Type the number of one of the items where you think agreeing with the item would mean the participant displayed autistic traits Now type the number of one of the items where you think disagreeing with the item would mean the participant displayed autistic traits For those items where agreeing with the item means a higher autistic quotient (AQ) score, participants recieve a score of 1 if they answer “Slightly agree” or “Agree”. This is called forward scoring. For those items where disagreeing with the item means a higher AQ score, participants recieve a score of 1 if they answer “Slightly disagree” or “Disagree”. This is know as reverse coding. The AQ score for each participant is the total score (i.e., the sum) of all 10 questions. The higher the AQ score, the more ’autistic traits’ they are assumed to exhibit and it is this score we are interested in. 5.1.3 Activity 1: Download the data Create a new folder for your Psych 1B data skills work. Do not call the folder “R” as this can cause R to have an existential crisis that it’s saving into itself. Download the Psych 1B zip file, extract the files, and then move the three csv files to the folder you created above. 5.1.4 Activity 2: Open a new Markdown document In Psych 1A, we provided the Markdown documents for you in the form of stub files. From this point on, you’re going to create and save your own. Open R Studio and set the working directory to your Psych 1B folder. If this has worked, you should see the csv files you just downloaded in the file pane in the bottom right of R Studio. To open a new R Markdown document click the ‘new item’ icon and then click ‘R Markdown’. You will be prompted to give it a title, call it “Lab 1 pre-class”. Also, change the author name to your GUID as this will be good practice for the homework. Keep the output format as HTML. Once you’ve opened a new document be sure to save it by clicking File -&gt; Save as. Name this file “Lab 1 pre-class”. If you’ve set the working directory correctly, you should now see this file appear in your file viewer pane. Figure 5.1: Opening a new R Markdown document 5.1.5 Activity 3: Create a new code chunk When you first open a new R Markdown document you will see a bunch of default welcome text. Do the following steps: Delete everything below line 7 On line 8 type “Activity 3” Click Insert -&gt; R Figure 5.2: New R chunk You should create a new code chunk for each activity or each analysis step and make sure there is a description of what the code is doing. This will make it easier to read your Markdown and find where any errors in the code are. Do not put all of your code in one big chunk. 5.1.6 Activity 4: Load in the data Type and run the code that loads the tidyverse package. Use read_csv() to load in the data. you should create three objects responses, scoring and qformats that contain the respective data. If you need help remembering how to load in data files, check Psych 1A, Lab 2 Activity 4. The solutions are at the bottom if you need them. However, remember what you learned about memory and learning in Psych 1A, you’ll learn more if you try and retrieve the code from memory. There were a couple of comments in the 1A EvaSys that said some of you felt like you were just copying and pasting - this can only be true if you choose to copy and paste. 5.1.7 Activity 5: Look at the data View each of the three datasets by clicking on their name in the environment. Check each object by using summary(), e.g., summary(qformats). responses contains the actual data from the survey. There is a participant Id column and then the rest of the columns contain the responses associated with that participant for each of the 10 questions (Q1, Q2, Q3, …, Q10). scoring contains the scoring information that we described above, that is, whether a question should be given a score of 1 or 0 for forward and reverse coded items for each possible response. qformats contains a list of whether each question is forward or reverse coded. 5.1.8 Thinking through the problem In order to get a total AQ score for each participant, we need to combine the information from all three files, that is, we need to know the participants response and then how to score it. We technically could score the data by hand. However, there are 66 participants in this dataset with each participant providing 10 responses. This means we would have to manually tidy up 660 responses. Not only would this be a horribly mind-numbing task, it is also one in which you would be prone to make errors. Even if you were 99% accurate, you would still get about 7 of the scores wrong. Worst of all, this approach does not scale beyond small datasets. When you analyse the data from your group project, there will likely be thousands of participants and you don’t have time to do these by hand! Pause here and think about how you might calculate AQ scores for each participant. What are the necessary steps? Let’s imagine we are doing the task by hand so that we understand the logic. Once that logic is clear, we’ll go through it again and show you how to write the script to make it happen. Let’s take stock of what we know. First, we know that there are two question formats, and that questions Q1, Q7, Q8, and Q10 are scored according to format F and questions Q2, Q3, Q4, Q5, Q6, and Q9 are scored according to format R. This is the information that is currently stored in qformats: Question QFormat Q 1 F Q 2 R Q 3 R Q 4 R Q 5 R Q 6 R Q 7 F Q 8 F Q 9 R Q 10 F We also know that for format F, we award a point for agree, zero for disagree. For format R, a point for disagree, zero for agree. This is the information that is currently stored in scoring: QFormat Response Score F Definitely Agree 1 F Slightly Agree 1 F Slightly Disagree 0 F Definitely Disagree 0 R Definitely Agree 0 R Slightly Agree 0 R Slightly Disagree 1 R Definitely Disagree 1 Finally, we would need to look at the responses from each participant and then give them the correct score for the format of the question. Let’s walk through the example with the first participant. For this participant (Id = 16), we have the following responses: Question Participant 16 Response Q 1 Slightly Disagree Q 2 Definitely Agree Q 3 Slightly Disagree Q 4 Definitely Disagree Q 5 Slightly Agree Q 6 Slightly Agree Q 7 Slightly Agree Q 8 Definitely Disagree Q 9 Slightly Agree Q 10 Slightly Agree Note that we have re-formatted the responses so that each response is in a separate row, rather than having all of the responses in a single row, as they are in responses. We have reshaped the data from its original wide format to long format. This format is called long because instead of having just one row for each participant, we now have one row for each data point and so 10 rows for each participant. While this format makes it less easy to read the whole dataset in with a single glance, it actually ends up being much easier to deal with, because ’Question’ is a now a single variable whose levels are Q1, Q2, …, Q10, and ’Response’ is also now a single variable. Most functions that you will be working with in R will expect your data to be in long rather than wide format. Let’s now look up the format for each question: Question Participant 16 Response QFormat Q 1 Slightly Disagree F Q 2 Definitely Agree R Q 3 Slightly Disagree R Q 4 Definitely Disagree R Q 5 Slightly Agree R Q 6 Slightly Agree R Q 7 Slightly Agree F Q 8 Definitely Disagree F Q 9 Slightly Agree R Q 10 Slightly Agree F And now that we have the format and the response, we can look up the scores: Question Participant 16 Response QFormat Score Q 1 Slightly Disagree F 0 Q 2 Definitely Agree R 0 Q 3 Slightly Disagree R 1 Q 4 Definitely Disagree R 1 Q 5 Slightly Agree R 0 Q 6 Slightly Agree R 0 Q 7 Slightly Agree F 1 Q 8 Definitely Disagree F 0 Q 9 Slightly Agree R 0 Q 10 Slightly Agree F 1 Then we just add up the scores, which yields an AQ score of 4 for participant 16. We would then repeat this logic for the remaining 65 participants. Anyone fancy doing this for a big data set?! Due to the large margin for error in carrying out this task by hand, it’s important to learn how to make the computer do the dirty work. The computer won’t make mistakes and will free up your mind to focus on the bigger issues in your research. 5.1.9 Activity 5: Recap Finally, let’s refresh your memory of some of the important tidyverse functions before the labs begin. What function would you use to keep just the columns Q1 and Q2 in responses? filter select object Q1:Q2 What function would you use to keep just the data from participant 16 in reponses? filter select summarise observation What function would you use to add up the total score for participant 16? filter inner_join summarise score What function would you use to join together qformats and scoring? inner_join join join_inner object_join 5.1.10 Activity 6: Reading and revision The final part of the pre-class involves some reading - head to the Lab 1 section of the 1B practical page on Moodle to get the materials. If you struggled with R last semester, please make sure that you revise the material from [1A - Lab 1])https://psyteachr.github.io/ug1-practical/a-lab-1.html#homework in particular as it explains a lot of the basics of R and it may help you to revisit this material. There will be an R catch-up session held by our of our GTAs Rebecca Lai on Wednesday 15th January Boyd Orr 603, 12:00 - 15:00. 5.1.11 Activity solutions 5.1.11.1 Activity 4 Solution library(tidyverse) scoring &lt;- read_csv (&quot;scoring.csv&quot;) responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) 5.2 In-class activities Let’s continue what we started in the pre-class activities by hand but now using R to calculate a score for each participant. 5.2.1 Activity 1: Load in the data Open a new R markdown document, name it “Lab 1 in-class” and save it in your Psych 1B folder. Set the working directory to your Psych 1B folder. Type and run the code that loads the tidyverse package. Use read_csv() to load in the data. you should create three objects responses, scoring and qformats that contain the respective data. 5.2.2 Activity 2: pivot_longer() The first step is to transform our data from wide format to long format. To do this, we will use the function pivot_longer(). pivot_longer() takes multiple columns and collapses them so that each unique variable has it’s own column and has four main arguments: data is the name of the object you want to transform names_to is the name of the new column that you will create that will contain the names of the original wide format columns values_to is the name of the column that will contain the existing values. cols are the original columns you want to collapse. These functions can seem a bit abstract and it is better to show than tell. Run the below code in a new code chunk and then compare how rlong looks compared to responses and see if you can figure out what effect each argument had. rlong &lt;- pivot_longer(data = responses, names_to = &quot;Question&quot;, values_to = &quot;Response&quot;, cols = Q1:Q10) You have now created a tibble with 660 observations and 3 variables; 10 observations per 66 participants and 3 variables. Let’s recreate the example from the pre-class only use one participant. We can do that by using filter() which you used last semester and creating a new tibble called rlong_16. 5.2.3 Activity 3: filter() Pause here and test your knowledge What does filter() do? Retains or removes variables Retains or removes observations Create a new object called rlong_16 Use filter() to keep only the data from participant Id 16. Every year, the biggest problem with these exercises is typos caused by not paying attention to the exact spelling and capitalisation. Remember, Question is not the same as question, Response is not the same as response, and Id is not the same as ID. 5.2.4 Activity 4: inner_join() The next step is to match each question with its format (F or R) that is stored in qformats. That is, we need to join together the two objects using inner_join() like we did in Psych 1A. Create a new object called rlong_16_join that uses inner_join() to join together rlong_16 and qformatsby their common column. If you get the error Error: by can't contain join column XXXX which is missing from LHS it means that you have made a typo. Check the exact spelling and capitalisation of the variable names. What inner_join() does is match up rows in the two tables where both tables have the same value for the field named in the third argument, “Question”; it then combines the columns from the two tables, copying rows where necessary. To state it more simply, what it does, in effect, is the following: For each row in rlong, it checks the value of the column Question, and looks for rows with the same value in qformats, and then essentially combines all of the other columns in the two tables for these matching rows. If there are unmatching values, the rows get dropped. The inner_join() is one of the most useful and time-saving operations in data wrangling so keep ptracticing as it will keep reappearing time after time. 5.2.5 Activity 5: Another inner_join() Now that we have matched up each question with its corresponding format, we can now “look up” the corresponding scores in the scoring table based on the format and the response. This means we have to use inner_join() once again to join rlong_16_join with scoring** Create a new object named scores_16 than joins together rlong_16_join with scoring. Be careful to tell R all of the columns the two objects have in common. Remember that when you need to specify multiple variables you will need to use the syntax by = c(\"var1\", \"var2). 5.2.6 Activity 6: Calculating the AQ score Now you need to calculate the total AQ score for participant 16. Create a new object called AQ_16. Use summarise() and sum()to add up the numbers in the column Score from scores_16 and call the result of this calculation AQ_score. This is quite a difficult task to do from memory but try it anyway - if you get anywhere near the right solution you’re doing extremely well! 5.2.7 Activity 7: Calculating all scores Next we’re going to do the same thing but for all participants. The first two steps are the same but we just use the full data rlong rather than the filtered dataset. Run the below code in a new code chunk. rlong_join &lt;- inner_join(rlong, qformats, &quot;Question&quot;) scores &lt;- inner_join(rlong_join, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) The final part of calculating the scores requires an extra step because now we don’t just want to calculate one score, we want to calculate a score for each participant which means that we need to use group_by() to group by Id. We’re not going to use it in this lab but we also want our object to show us the participant’s gender so we will also add gender to the grouping. If you want to refresh your memory about how group_by() works, revise Psych 1A Lab 3 Activity 8. scores_grouped &lt;- group_by(scores, Id, gender) AQ_all &lt;- summarise(scores_grouped, total_score = sum(Score)) 5.2.8 Activity 8: Visualisation Finally, use ggplot() and geom_histogram() to make a histogram of all the total AQ scores. Try and make it look pretty by changing the axis labels and the theme. You can check the solution code to see how the below example was made, but you can make yours look different. Hint 1: ggplot(data, aes(x)) + geom_histogram() Hint 2: Add binwidth = 1 to geom_histogram() to change the width of the bars. Figure 5.3: Histogram of total AQ scores 5.2.9 Activity solutions 5.2.9.1 Activity 1 Solution library(tidyverse) scoring &lt;- read_csv (&quot;scoring.csv&quot;) responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) 5.2.9.2 Activity 3 Solution rlong_16 &lt;- filter(rlong, Id == 16) 5.2.9.3 Activity 4 Solution rlong_16_join &lt;- inner_join(rlong_16, qformats, &quot;Question&quot;) 5.2.9.4 Activity 5 Solution scores_16 &lt;- inner_join(rlong_16_join, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 5.2.9.5 Activity 6 Solution AQ_16 &lt;- summarise(scores_16, AQ_score = sum(Score)) 5.2.9.6 Activity 8 Solution ggplot(AQ_all, aes(x = total_score)) + geom_histogram(binwidth = 1, colour = &quot;black&quot;, fill = &quot;grey&quot;) + theme_minimal()+ scale_x_continuous(name = &quot;Total AQ Score&quot;, breaks = c(0,1,2,3,4,5,6,7,8,9,10)) + scale_y_continuous(name = &quot;Count&quot;) "],
["b-lab-2.html", "6 1B: Lab 2 6.1 Pre-class activities 6.2 In-class activities", " 6 1B: Lab 2 6.1 Pre-class activities 6.1.1 Activity 1: Recap In Psych 1A we briefly introduced the concept of pipes, if you’d like to recap what you did, you can find it here. Pipes look like this: %&gt;%. Pipes allow you to send the output from one function straight into another function. Specifically, they send the result of the function before %&gt;% to be the first argument of the function after %&gt;%. As usual, it’s easier to show, rather than tell so let’s look at an example. In the below example we want to perform some common wrangling steps: Read in the data Select the columns we want to retain Join together our data files Filter out participants so that we just use data from people who are aged 18 years or older So far, when you’ve done these steps you’ve created a new object for each part of the process. The code below should look familiar - once you’ve loaded in the data to demographic and raw_data, you then create a new object named sel_data that stores the output of the select() operation, then you create a new object named joined_data that uses sel_data as its input and then finally you create another object called final_data that uses joined_data as its input. demographic &lt;- read_csv(&quot;demographic.csv&quot;) raw_data &lt;- read_csv(&quot;my_data.csv&quot;) sel_data &lt;- select(raw_data, id, score) joined_data &lt;- inner_join(sel_data, demographic, by = &quot;id&quot;) final_data &lt;- filter(joined_data, age &gt;= 18) This code will work, however, there’s a more efficient way of doing all of these steps that needs far less typing which means fewer chances to make typos. Additionally, if you create lots of objects, it increases the chances that you might accidentally use the wrong one (e.g., rlong instead of rlong2). By using pipes, we can reduce the amount of code we write and the number of objects we create. Remember, pipes allow you to send the output from one function straight into another function where they are used as the first argument. Try and translate the below code into plain English. You can read the %&gt;% as then. demographic &lt;- read_csv(&quot;demographic.csv&quot;) raw_data &lt;- read_csv(&quot;my_data.csv&quot;) final_data &lt;- raw_data %&gt;% select(id, score) %&gt;% inner_join(demographic, by = &quot;id&quot;) filter(age &gt;= 18) Solution Take the object raw_data then select the columns id and score then join it with the data in demographic by id then filter to only include ages of 18 or greater and save this in an object named final_data. Importantly, you don’t need to write the entire pipe at once, in fact, it’s useful to check that each line works as you intended and then add on the next one to make it easier to spot where errors have come in to your code. For example you might start with: final_data &lt;- raw_data %&gt;% select(id, score) Then check the output of final_data to make sure that it contains the columns you intended to select. Then you would add on another line of the pipe: final_data &lt;- raw_data %&gt;% select(id, score) %&gt;% inner_join(demographic, by = &quot;id&quot;) And check the output again to make sure that the join had worked properly before adding on the final line. final_data &lt;- raw_data %&gt;% select(id, score) %&gt;% inner_join(demographic, by = &quot;id&quot;) filter(age &gt;= 18) As great as pipes are, there are two main cases when it probably makes sense to write your code without pipes, and save the results from each step to separate objects: When the output from a step partway through your pipeline is somehow useful. For example, you might want to check that a step worked as expected, or the information might be useful again later on. When you’re completing assessments and we want you to save the output from a certain step to a specific variable name (we want to check that the step worked properly to be able to give you the mark!). Which of the following is a pipe? %\"]'> %&lt;% &lt;- %&gt;% Where do pipes send the result of the function that precedes the pipe? To the last argument of the next function. To the first argument of the next function. To the third argument of the 18th function. 6.1.2 Activity 2: Set-up We’re going to load in the AQ data to practice some examples. Open a new R Markdown document, name it “Lab 2 pre-class” and save it in your Psych 1B folder. Make sure the working directory is set to your Psych 1B folder that has the three AQ .csv files. Delete the welcome text and insert a new code chunk. Copy, paste, and run the below code into the new code chunk. library(tidyverse) scoring &lt;- read_csv (&quot;scoring.csv&quot;) responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) 6.1.3 Activity 3: Example pipe 1 Rewrite the below code to use pipes. The solutions are at the bottom but try and work through it on your own - it can be a bit confusing at first but there will be a moment where it suddenly makes sense and you will find it much easier to write code. Remember that if something is difficult it probably means you’re learning more. Hint 1: You’re not creating new objects so you can delete the intermediate object names and save the output straight into the final object name scores. Hint 2: The point of using pipes is that they pipe the output into the first argument of the next function, in other words, you can delete the first argument of the new function. Hint 3: Build the pipe up line-by-line and check it at each stage. The output of the first line of the pipe should look like rlong, the output of the second line should look like rlong_join and so on. rlong &lt;- pivot_longer(data = responses, names_to = &quot;Question&quot;, values_to = &quot;Response&quot;, cols = Q1:Q10) rlong_join &lt;- inner_join(rlong, qformats, &quot;Question&quot;) scores &lt;- inner_join(rlong_join, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 6.1.4 Activity 4: Example pipe 2 Rewrite the below code to use pipes. data1 &lt;- select(scores, -QFormat) data2 &lt;- filter(data1, Question == &quot;Q1&quot;) example_2 &lt;- summarise(data2, total_score = sum(Score)) 6.1.5 Activity 5: Example pipe 3 You can also pipe into ggplot(). The below example calculates the total score for each question (not each participant) by grouping it, then using summarise on the grouped data, then making a graph of these scores so that we can see which question participants were most likely to give an answer indicating autistic traits. Rewrite the below code using pipes. Hint 1: You do not need to save this output to an object. Hint 2: First, write the code that calculates the scores and then pipe into gggplot() Hint 3: The pipe works exactly the same with ggplot(), you can delete the first argument. data_group &lt;- group_by(scores, Question) question_scores &lt;- summarise(data_group, item_scores = sum(Score)) ggplot(question_scores, aes(x = Question, y = item_scores)) + geom_col() + theme_minimal() Figure 4.9: Total scores for each AQ question 6.1.6 Activity 6: Reading You should also read the ethics document which is available in the Lab 2 section of the 1B Practical Moodle. 6.1.7 Activity solutions 6.1.7.1 Activity 3 Solution scores &lt;- pivot_longer(data = responses, names_to = &quot;Question&quot;, values_to = &quot;Response&quot;, cols = Q1:Q10) %&gt;% inner_join(qformats, &quot;Question&quot;) %&gt;% inner_join(scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) # this also does the same thing but the first line is slightly different scores &lt;- responses %&gt;% pivot_longer(names_to = &quot;Question&quot;, values_to = &quot;Response&quot;, cols = Q1:Q10) %&gt;% inner_join(qformats, &quot;Question&quot;) %&gt;% inner_join(scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 6.1.7.2 Activity 4 Solution example_2 &lt;- select(scores, -QFormat) %&gt;% filter(Question == &quot;Q1&quot;) %&gt;% summarise(total_score = sum(Score)) 6.1.7.3 Activity 5 Solution scores %&gt;% group_by(Question) %&gt;% summarise(item_scores = sum(Score)) %&gt;% ggplot(aes(x = Question, y = item_scores)) + geom_col() + theme_minimal() 6.2 In-class activities 6.2.1 Activity 1: Set-up Open a new R markdown document, name it “Lab 2 in-class” and save it in your Psych 1B folder. Set the working directory to your Psych 1B folder. Type and run the code that loads the tidyverse package. Use read_csv() to load in the data. you should create three objects responses, scoring and qformats that contain the respective data. 6.2.2 Activity 2: Mega pipe We’re going to build on what you learned in the pre-class and rewrite all of the code we did in Lab 1 (which you can see below) using a pipe. As a reminder, the code: Transforms the data from wide-form to long-form Joins together the three objects by their common columns Calculates a total AQ score for each participant rlong &lt;- pivot_longer(data = responses, names_to = &quot;Question&quot;, values_to = &quot;Response&quot;, cols = Q1:Q10) rlong_join &lt;- inner_join(rlong, qformats, by = &quot;Question&quot;) scores &lt;- inner_join(rlong_join, scoring, by = c(&quot;QFormat&quot;, &quot;Response&quot;)) scores_grouped &lt;- group_by(scores, Id, gender) AQ_all &lt;- summarise(scores_grouped, total_score = sum(Score)) Rewrite the above code using pipes %&gt;%. Make sure you have completed the pre-class before you attempt this. 6.2.3 Activity 3: Pipe plot Now we’ve got our total AQ scores, let’s use the pipe to make a graph. Take AQ_all then filter it leaving only female participants then use ggplot() to create a histogram of the total scores (you did this in 1B Lab 1). If you’ve done it correctly, it should look like the below (you can change the colours if you like). Figure 6.1: Histogram of scores for female participants Remember this for your group project - rather than creating new objects for each graph you want to make you can just pipe the data you want to display straight into ggplot(). 6.2.4 Activity 4: AQ score by gender More men and boys are currently diagnosed as autistic than women and girls and there is increasing evidence that there is an over-representation of transgender and nonbinary people in those with autism spectrum disorder (ASD) or who meet the AQ cut-off score for ASD, therefore, it seems sensible to visualise AQ scores by gender (note that this dataset is simulated and that whilst the pattern of results is based on what we would expect from the evidence, these are not real data). Using the data in AQ_all, create a violin-boxplot of the total AQ scores by gender. Hint: gender should be on the x-axis, total_score should be on the y-axis. You can use a pipe if you want, but it doesn’t make much difference in this activity. Look at the graph and answer the following questions: Which group has the lowest median total AQ score? female male nonbinary Which group has an outlier? female male nonbinary Which of the following do you think would be an accurate conclusion to draw from the plot? Women had higher AQ scores than men or nonbinary people Men had higher AQ scores than nonbinary people who had higher scores than women Men and nonbinary people’s scores did not differ much and both had higher AQ scores than women 6.2.5 Activity 5: Bad bar plots In the research methods lectures for Psych 1A, we talked about the importance of data visualisation and how different graphs might lead you to make very different conclusions about your data. For this reason, we’ve taught you how to make violin-boxplots because these show the true distribution of the data, however, it’s useful to know how to make bad bar plots so that you can see the difference they make to your own data (but never use them as your only method of visualisation!). Copy, paste and run the below code in a new code chunk Making a bar chart works a little differently to the other graphs you have made so far. Previously, ggplot() has just visualised the raw data, however, for a bar chart you actually want to visualise a summary of the data, e.g., the mean. You can read the stat_summary code as “draw a summary of the data, use a bar chart to do so and the function to display on the y-axis should be the mean. Then, draw another summary but this time use an errorbar and the function to apply to the data is standard error of the mean”. AQ_all %&gt;% ggplot(aes(x = gender, y = total_score)) + stat_summary(geom = &quot;bar&quot;, fun.y = &quot;mean&quot;) + stat_summary(geom = &quot;errorbar&quot;, fun.data = &quot;mean_se&quot;, width = .2) + theme_minimal() Figure 6.2: Bad bar chart of means Think back to your interpretation of the violin-boxplot, that men and nonbinary people’s scores did not differ much and both had higher AQ scores than women. Would you have concluded the same thing if you had looked at the bar chart? In this dataset, the outlier in the male group results in the mean score being much higher than the nonbinary mean, it’s only through looking at the full distribution with the violin-boxplot that you can accurately intepret the data. 6.2.6 Activity 6: Density plots The final type of visualiation we’re going to show you are density plots as they are a useful way of visualising how the distributions of different groups compare to each other. You’ve actually already seen a density plot - it’s the base of a violin plot, however, it can be useful to overlap them. Copy, paste, and run the below code in a new code chunk. A lot of this code should be familiar to you, most of it is editing the axis labels and the theme. Adding fill = gender tells ggplot to produce different coloured geoms for each level of gender. Try removing fill = gender and see what happens to the plot. geom_density() is our new geom and tells R to draw the density curve. The argument alpha controls the transparency of the colours, try adjusting this value. ggplot(AQ_all, aes(x = total_score, fill = gender)) + geom_density(alpha = .3) + theme_minimal() + scale_fill_viridis_d(option = &quot;D&quot;) + scale_x_continuous(name = &quot;Total AQ Score&quot;, breaks = c(1,2,3,4,5,6,7,8,9,10)) + scale_y_continuous(name = &quot;Density&quot;) Figure 6.3: Grouped density plot The y-axis displays density, i.e., what proportion of the data points fall at eaach point on the x-axis. Approximately what percent of female participants had a total AQ of 2? .3% 3% 30% Solution It’s important to be able to translate between proportions and percentages, it will make your understanding of statistics and p-values much easier. To translate a proportion into a percentage, you multiply by 100 or move the decimal place two places to the right so a proportion of .5 = 50%, a proportion of .03 = 3% and so on. 6.2.7 Activity 7: Saving plots Finally, it’s useful to be able to save a copy of your plots as an image file so that you can use it in a presentation or word document and to do this we can use the function ggsave(). There are two ways you can use ggsave(). If you don’t tell ggsave() which plot you want to save, by default it will save a copy of the last plot you created. If you’ve been following this chapter in order, the last plot you created should have been the density plot from Activity 6. All that ggsave() requires is for you to tell it what file name it should save the plot to and the type of image file you want to create (the below example uses .png but you could also use e.g., .jpeg and other image types). Copy, paste and run the below code into a new code chunk. Check your Psych 1B folder, you should now see the saved image file. ggsave(&quot;density.png&quot;) The default size for an image is 7 x 7, you can change this manually if you think that the dimensions of the plot are not correct or if you need a particular size. Copy paste and run the below code to overwrite the image file with new dimensions. ggsave(&quot;density.png&quot;, width = 10, height = 8) The second way of using ggsave() is to save your plot as an object and then tell it which object to write to a file. The below code saves the pipe plot from Activity 3 into an object named AQ_histogram and then saves it to an image file “AQ_histogram.png”. Note that when you save a plot to an object, it won’t display in R Studio. To get it to display you need to type the object name in the console (i.e., AQ_histogram). The benefit of doing it this way is that if you are making lots of plots, you can’t accidentally save the wrong one because you are explicitly specifying which plot to save rather than just saving the last one. Copy, paste and run the below code and then check your Psych 1B folder for the image file. Resize the plot if you think it needs it. AQ_histogram &lt;- AQ_all %&gt;% filter(gender == &quot;female&quot;) %&gt;% ggplot(aes(x = total_score)) + geom_histogram(binwidth = 1, colour = &quot;black&quot;, fill = &quot;grey&quot;) + theme_minimal()+ scale_x_continuous(name = &quot;Total AQ Score (female participants)&quot;, breaks = c(0,1,2,3,4,5,6,7,8,9,10)) + scale_y_continuous(name = &quot;Count&quot;) ggsave(&quot;AQ_histogram.png&quot;, plot = AQ_histogram) 6.2.8 Activity solutions 6.2.8.1 Activity 1 Solution library(tidyverse) scoring &lt;- read_csv (&quot;scoring.csv&quot;) responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) 6.2.8.2 Activity 2 Solution AQ_all &lt;- pivot_longer(data = responses, names_to = &quot;Question&quot;, values_to = &quot;Response&quot;, cols = Q1:Q10) %&gt;% inner_join(qformats, by = &quot;Question&quot;) %&gt;% inner_join(scoring, by = c(&quot;QFormat&quot;, &quot;Response&quot;)) %&gt;% group_by(Id, gender) %&gt;% summarise(total_score = sum(Score)) 6.2.8.3 Activity 3 Solution AQ_all %&gt;% filter(gender == &quot;female&quot;) %&gt;% ggplot(aes(x = total_score)) + geom_histogram(binwidth = 1, colour = &quot;black&quot;, fill = &quot;grey&quot;) + theme_minimal()+ scale_x_continuous(name = &quot;Total AQ Score (female participants)&quot;, breaks = c(0,1,2,3,4,5,6,7,8,9,10)) + scale_y_continuous(name = &quot;Count&quot;) 6.2.8.4 Activity 4 Solution ggplot(AQ_all, aes(gender, total_score)) + geom_violin(trim = FALSE) + geom_boxplot(width = .2) + theme_minimal() "],
["b-lab-3.html", "7 1B: Lab 3 7.1 Pre-class activities 7.2 In-class activities", " 7 1B: Lab 3 7.1 Pre-class activities In the Psych 1A Research Methods lectures, we talked a lot about p-values and statistical significance. P-values are the probability that you would get the observed results if the null hypothesis was true (i.e., if there really was no effect of your experiment). In psychology, the standard cut-off for statistical significance is p &lt; .05, that is, if the probability that we would observe our results under the null hypothesis is less than 5%, we conclude that our experiment has had an effect and there is a difference between our groups. In this lab, we’re going to go into a bit more detail about exactly what we mean by probability in preparation for starting statistics in level 2. 7.1.1 Activity 1: Prep The pre-class for this week doesn’t involve doing any coding in R, instead, we’re going to cover some core statistical concepts. Read Statistical thinking (Noba Project) Watch Normal Distribution - Explained Simply (10 mins) Watch Probability explained (8 mins) Watch Binomial distribution (12 minutes) 7.1.2 What is probability? Probability (p) is the extent to which an event is likely to occur and is represented by a number between 0 and 1. For example, the probability of flipping a coin and it landing on ‘heads’ would be estimated at p = .5, i.e., there is a 50% chance of getting a head when you flip a coin. Calculating the probability of any discrete event occurring can be formulated as: \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] For example, what is the probability of randomly drawing your name out of a hat of 12 names where one name is definitely yours? 1/12 ## [1] 0.08333333 The probability is .08, or to put it another way, there is an 8.3% chance that you would pull your name out of the hat. 7.1.3 Types of data How you tackle probability depends on the type of data/variables you are working with (i.e. discrete or continuous). This is also referred to as Level of Measurements. Discrete data can only take integer values (whole numbers). For example, the number of participants in an experiment would be discrete - we can’t have half a participant! Discrete variables can also be further broken down into nominal and ordinal variables. Ordinal data is a set of ordered categories; you know which is the top/best and which is the worst/lowest, but not the difference between categories. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). You know that very attractive is better than attractive but we can’t say for certain that the difference between neutral and attractive is the same size as the distance between very unattractive and unattractive. Nominal data is also based on a set of categories but the ordering doesn’t matter (e.g. left or right handed). Nominal is sometimes simply referred to as categorical data. Continuous data on the other hand can take any value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), other examples include reaction time or the distance you travel to university every day. Continuous data can be broken down into interval or ratio data. Interval data is data which comes in the form of a numerical value where the difference between points is standardised and meaningful. For example temperature, the difference in temperature between 10-20 degrees is the same as the difference in temperature between 20-30 degrees. Ratio data is very like interval but has a true zero point. With our interval temperature example above, we have been experiencing negative temperatures (-1,-2 degrees) in Glasgow but with ratio data you don’t see negative values such as these i.e. you can’t be -10 cm tall. 7.1.4 Activity 2: Types of data What types of data are the below measurements? Time taken to run a marathon (in seconds): ordinal categorical ratio interval Finishing position in marathon (e.g. 1st, 2nd, 3rd): ratio categorical interval ordinal Which Sesame Street character a runner was dressed as: categorical interval ordinal ratio Temperature of a runner dressed in a cookie monster outfit (in degrees Celsius): ordinal ratio interval categorical 7.1.5 Probability distributions Probability distribution is a term from mathematics. Suppose there are many events with random outcomes (e.g., flipping a coin). A probability distribution is the theoretical counterpart to the observed frequency distribution. A frequency distribution simply shows how many times a certain event actually occurred. A probability distribution says how many times it should have occurred. Mathematicians have discovered a number of different probability distributions, that is, we know that different types of data will tend to naturally fall into a known distribution and we can use them to help us calculate probability. 7.1.6 The uniform distribution The uniform distribution is when each possible outcome has an equal chance of occurring. Let’s take the example from above, pulling your name out of a hat of 12 names. Each name has an equal chance of being drawn (p = .08). If we visualised this distribution, it would look like this - each outcome has the same chance of occurring: Figure 7.1: Uniform distribution 7.1.7 The binomial distribution The binomial (bi = two, nominal = categories) distribution is a frequency distribution which calculates probabilities of success for situations where there are two possible outcomes e.g., flipping a coin. A binomial distribution models the probability of any number of successes being observed, given the probability of a success and the number of observations. Binomial distributions represent discrete data. Describing the probability of single events, such as a single coin flip or rolling a six is easy, but more often than not we are interested in the probability of a collection of events, such as the number of heads out of 10 coin flips. To work this out, we can use the binomial distribution and functions in R. Let’s say we flip a coin 10 times. Assuming the coin is fair (probability of heads = .5), how many heads should we expect to get? The below figure shows the results of a simulation for 10,000 coin flips (if you’d like to do this simulation yourself in R, you can see the code by clicking “Solution”). What this means is that we can use what we know about our data and the binomial distribution to work out the probability of different outcomes (e.g., what’s the probability of getting at least 3 heads if you flip a coin 10 times?) and this is what we’ll do in the lab. Figure 1.1: Probability of no. of heads from 10 coin tosses Solution heads10000 &lt;- replicate(n = 10000, expr = sample(0:1, 10, TRUE) %&gt;% sum()) data10000 &lt;- tibble(heads = heads10000) %&gt;% # convert to a tibble group_by(heads) %&gt;% # group by number of possibilities summarise(n = n(), # count occurances of each possibility, p=n/10000) # &amp; calculate probability (p) of each ggplot(data10000, aes(x = heads,y = p)) + geom_bar(stat = &quot;identity&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 10 flips (p)&quot;) + theme_bw() + scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10)) 7.1.8 The normal distribution The final probability distribution you need to know about is the normal distribution. The normal distribution, reflects the probability of any value occurring for a continuous variable. Examples of continuous variables include height or age, where a single person can score anywhere along a continuum. For example, a person could be 21.5 years old and 176cm tall. As the normal distribution models the probability of a continuous variable, we plot the probability using a density plot. A normal distribution looks like this: Figure 4.4: Normal Distribution of height. \\(\\mu\\) = the mean (average), \\(\\sigma\\) = standard deviation Normal distributions are symmetrical, meaning there is an equal probability of observations occurring above and below the mean. This means that, if the mean in figure 1 is 170, we could expect the number of people who have a height of 160 to equal the number of people who have a height of 180. This also means that the mean, median, and mode are all expected to be equal in a normal distribution. In the same way that we could with the coin flips, we can then use what we know about our data and the normal distribution to estimate the probability of certain outcomes (e.g., what’s the probability that someone would be taller than 190cm?) and we’ll do this in the lab. As with any probabilities, real-world data will come close to the normal distribution, but will (almost certainly) never match it exactly. As we collect more observations from normally-distributed data, our data will get increasingly closer to a normal distribution. As an example, here’s a simulation of an experiment in which we collect heights from 5000 participants. As you can see, as we add more observations, our data starts to look more and more like the normal distribution in the previous figure. Figure 4.5: A simulation of an experiment collecting height data from 2000 participants 7.1.9 Activity 3: Normal distribution Complete the sentences so that they are correct. In a normal distribution, the mean, median, and mode are always different are all equal sum to zero. In a normal distribution, the further away from the mean an observation is the lower its probability of occuring the higher its probability of occuring. Whereas the binomial distribution is based on situations in which there are two possible outcomes, the normal distribution is based on situations in which the data is a continuous variable is a categorical variable has three possible values. 7.1.10 Activity 4: Distribution test Which distribution is likely to be associated with the following? Scores on an IQ test Uniform distribution Binomial distribution Normal distribution Whether a country has won or lost the Eurovision song contest Uniform distribution Binomial distribution Normal distribution Picking a spade card out of a normal pack of playing cards Uniform distribution Binomial distribution Normal distribution In the labs we’re going to continue looking at distributions and probability. Whilst you won’t start conducting statistical tests until level 2, by the end of the lab you should be able to understand the core principles of probability and how we can use what we know about distributions to calculate whether a particular outcome is likely. 7.2 In-class activities 7.2.1 Activity 1: Binomial distribution First, we’re going to calculate probabilities based on the binomial distribution. This week, for the first time we don’t need to load the tidyverse. All of the functions we need are contained in Base R. If you want a refresher on the difference between Base R and packages, see Psych 1A lab 1. Open a new R Markdown document, call it “Lab 3 in-class” and save it in your Psych 1B folder. We’re going to use three Base R functions to work with the binomial distribution: dbinom() - the density function: gives you the probability of x successes given the number of trials and the probability of success on a single trial (e.g., what’s the probability of flipping 8/10 heads with a fair coin?). pbinom() - the probability distribution function: gives you the cumulative probability of getting a number of successes below a certain cut-off point (e.g. probability of getting 0 to 5 heads out of 10 flips), given the size and the probability. This is known as the cumulative probability distribution function or the cumulative density function. qbinom() - the quantile function: is the opposite of pbinom() in that it gives you the x axis value for a given probability p, plus given the size and prob, that is if the probability of flipping a head is .5, how many heads would you expect to get with 10 flips? So let’s try these functions out to answer two questions: What is the probability of getting exactly 5 heads on 10 flips? What is the probability of getting at most 2 heads on 10 flips? 7.2.2 Activity 2: dbinom() Let’s start with question 1, what is the probability of getting exactly 5 heads on 10 flips? We want to predict the probability of getting 5 heads in 10 trials (coin flips) and the probability of success is 0.5 (it’ll either be heads or tails so you have a 50/50 chance which we write as 0.5). We will use dbinom() to work this out: The dbinom() (density) function has three arguments: x: the number of ‘heads’ we want to know the probability of. Either a single one, 3 or a series 0:10. In this case it’s 5. size: the number of trials (flips) we are simulating; in this case, 10 flips. prob: the probability of ‘heads’ on one trial. Here chance is 50-50 which as a probability we state as 0.5 or .5 Copy, paste and run the below code in a new code chunk: dbinom(x = 5, size = 10, prob = 0.5) What is the probability of getting 5 heads out of 10 coin flips to 2 decimal places? What is this probability expressed in percent? 0.25% 2.5% 25% 7.2.3 Activity 3: pbinom() OK, question number 2. What is the probability of getting at most 2 heads on 10 flips? This time we use pbinom() as we want to know the cumulative probability of getting a maximum of 2 heads from 10 coin flips. so we have set a cut-off point of 2 but we still have a probability of getting a heads of 0.5. Note: pbinom() takes the arguments size and prob argument just like dbinom(). However, the first input argument is q rather than x. This is because in dbinom x is a fixed number, whereas q is all the possibilities up to a given number (e.g. 0, 1, 2). Copy, paste and run the below code in a new code chunk: pbinom(q = 2, size = 10, prob = 0.5) What is the probability of getting a maximum of 2 heads on 10 coin flips to 2 decimal places? What is this probability expressed in percent? 0.06% 0.6% 6% 7.2.4 Activity 4: pbinom() 2 Let’s try one more scenario with a cut-off point to make sure you have understood this. What is the probability of getting 7 or more heads on 10 flips? We can use the same function as in the previous example, however, there’s an extra argument if we want to get the correct answer. Let’s try running the code we used above but change q = 2 to q = 7. pbinom(q = 7, size = 10, prob = .5) ## [1] 0.9453125 This tells us that the probability is .95 or 95% - that doesn’t seem right does it? The default behaviour for pbinom() is to calculate cumulative probability for the lower tail of the curve, i.e., if you specify q = 2 it calculates the probability of all outcomes below 2. We specified q = 7 which means that it’s calculated the probability of getting an outcome of 0, 1, 2, 3, 4, 5, 6, and 7, the blue area in the below figure - which is obviously very high. Figure 4.6: Lower and upper tails To get the right answer, we have to add lower.tail = FALSE as we are interested in the upper tail of the distribution. Because we want the cumulative probability to include 7, we set q = 6. This will now calculate the cumulative probability of getting 7, 8, 9, or 10 heads out of 10 coin flips. Copy, paste and run the below code in a new code chunk: pbinom(q = 6, size = 10, prob = .5, lower.tail = FALSE) What is the probability of getting between 7 and 10 heads from 10 coin flips to 2 decimal places? What is this probability expressed in percent? 0.017% 0.17 17% 7.2.5 Activity 5: qbinom() OK, now let’s consider a scenario in which you’d use the quantile function qbinom. Imagine that you’ve been accosted by a street magician and they want to bet you that they can predict whether the coin will land on heads or tails. You suspect that they’ve done something to the coin so that it’s not fair and that the probability of the coin landing on a head is no longer .5 or 50/50, it’s now more likely to land on tails. Your null hypothesis is that the coin is not dodgy and that the probability should be even (P(heads)=.5).You are going to run a single experiment to test your hypothesis, with 10 trials. What is the minimum number of heads that is acceptable if p really does equal .5? You have used the argument prob in the previous two functions, dbinom and pbinom, and it represents the probability of success on a single trial (here it is the probability of ‘heads’ in one coin flip, .5). For qbinom, prob still represents the probability of success in one trial, whereas p represents the overall probability of success across all trials. When you run pbinom, it calculates the number of heads that would give that probability. We know from looking at the binomial distribution above that sometimes even when the coin is fair, we won’t get exactly 5/10 heads. Instead, we want to set a cut-off and in this example we will use the default cut-off for statistical significance in psychology, .05, or 5%. In other words, you ask it for the minimum number of successes (e.g. heads) to maintain an overall probability of .05, in 10 flips, when the probability of a success on any one flip is .5. qbinom(p = .05, size = 10, prob = .5) ## [1] 2 And it tells you the answer is 2. If the magician flipped fewer than two heads out of ten, you could conclude that there is a less than 5% probability that would happen if the coin was fair and you would reject the null hypothesis that the coin was unbiased against heads and tell the magician to do one. qbinom also uses the lower.tail argument and it works in a similar fashion to pbinom. However, ten trials is probably far too few if you want to accuse the magician of being a bit dodge. Run the below code and then answer the following questions: What would your cut-off be if you ran 100 trials? What would your cut-off be if you ran 1000 trials? What would your cut-off be if you ran 10,000 trials? qbinom(p = .05, size = c(100, 1000, 10000), prob = .5) Notice that the more trials you run, the more precise the estimates become, that is, the closer they are to the probability of success on a single flip (.5). Again this is a simplification, but think about how this relates to sample size in research studies, the more participants you have, the more precise your estimate will be. Visualise it! Have a go at playing around with different numbers of coin flips and probabilities in our dbinom() and pbinom() app! Figure 4.7: Binomial distribution app 7.2.6 Normal distribution A similar set of functions exist to help us work with other distributions, including the normal distribution and we’re going to use three of these: dnorm()-the density function, for calculating the probability of a specific value pnorm()-the probability or distribution function, for calculating the probability of getting at least or at most a specific value qnorm()-the quantile function, for calculating the specific value associated with a given probability As you can probably see, these functions are very similar to the functions we’ve already come across, that are used to work with the binomial distribution. 7.2.7 Probability of heights Data from the Scottish Health Survey (2008) shows that: The average height of a 16-24 year old Scottish man is 176.2 centimetres, with a standard deviation of 6.748. The average height of a 16-24 year old Scottish woman is 163.8 cm, with a standard deviation of 6.931. There are currently no data on Scottish trans and non-binary people. The below figure is a simulation of this data - you can see the code used to run this simulation by clicking the solution button. Solution men &lt;- rnorm(n = 100000, mean = 176.2, sd = 6.748) women &lt;- rnorm(n = 100000, mean = 163.8, sd = 6.931) heights &lt;- tibble(men, women) %&gt;% pivot_longer(names_to = &quot;sex&quot;, values_to = &quot;height&quot;, men:women) ggplot(heights, aes(x = height, fill = sex)) + geom_density(alpha = .6) + scale_fill_viridis_d(option = &quot;E&quot;) + theme_minimal() Figure 1.6: Simulation of Scottish height data In this lab we will use this information to calculate the probability of observing at least or at most a specific height with pnorm(), and the heights that are associated with specific probabilities with qnorm(). 7.2.8 Activity 6:pnorm() pnorm() requires three arguments: q is the value that you want to calculate the probability of mean is the mean of the data sd is the standard deviation of the data lower.tail works as above and depends on whether you are interested in the upper or lower tail pnorm(q = NULL, mean = NULL, sd = NULL, lower.tail = NULL) Replace the NULLs in the above code to calculate the probability of meeting a 16-24 y.o. Scottish woman who is taller than the average 16-24 y.o. Scottish man. What is the probability of meeting a 16-24 y.o. Scottish woman who is taller than the average 16-24 y.o. Scottish man? What is this probability expressed in percent? 0.04% 0.4% 4% 7.2.9 Activity 7: pnorm 2 Fiona is a very tall Scottish woman (181.12cm) in the 16-24 y.o. range who will only date men who are taller than her. Using pnorm() again, what is the proportion of Scottish men Fiona would be willing to date to 2 decimal places? What is this probability expressed in percent? 0.23% 2.3% 23% 7.2.10 Activity 8: pnorm 3 On the other hand, Fiona will only date women who are shorter than her. What is the proportion of Scottish women would Fiona be willing to date to 2 decimal places? What is this probability expressed in percent? 0.99% 9.9% 99% 7.2.11 Activity 9: qnorm() In the previous examples we calculated the probability of a particular outcome. Now we want to calculate what outcome would be associated with a particular probability and we can use qnorm() to do this. qnorm() is very similar to pnorm() with one exception, rather than specifying q our known observation or quantile, instead we have to specify p, our known probability. qnorm(p = NULL, mean = NULL, sd = NULL, lower.tail = NULL) Replace the NULLs in the above code to calculate how tall a 16-24 y.o. Scottish man would have to be in order to be in the top 5% (i.e., p = .05) of the height distribution for Scottish men in his age group. Visualise it! Have a go at playing around with different distributions in our dnorm() and pnorm() app - access it here And that’s it! The key concepts to take away from this lab are that different types of data tend to follow known distributions, and that we can use these distributions to calculate the probability of particular outcomes. This is the foundation of many of the statistical tests that you will learn about in level 2. For example, if you want to compare whether the scores from two groups are different, that is, whether they come from different distributions, you can calculate the probability that the scores from group 2 would be in the same distribution as group 1. If this probability is less than 5% (p = .05), you might conclude that the scores were significantly different. That’s an oversimplification obviously, but if you can develop a good understanding of probability distributions it will stand you in good stead for level 2. 7.2.12 Activity solutions 7.2.12.1 Activity 2 Solution .25 7.2.12.2 Activity 3 Solution .06 7.2.12.3 Activity 4 Solution .17 7.2.12.4 Activity 5 Solution pnorm(q = 176.2, mean = 163.8, sd = 6.931, lower.tail = FALSE) 7.2.12.5 Activity 6 Solution pnorm(q = 181.12, mean = 176.2, sd = 6.748, lower.tail = FALSE) 7.2.12.6 Activity 7 Solution pnorm(q = 181.12, mean = 163.8, sd = 6.931, lower.tail = TRUE) 7.2.12.7 Activity 8 Solution qnorm(p = .05, mean = 176.2, sd = 6.748, lower.tail = FALSE) "],
["b-lab-4.html", "8 1B: Lab 4 8.1 Pre-class activities 8.2 In-class activities", " 8 1B: Lab 4 8.1 Pre-class activities For your final Level 1 pre-class activity (hurrah!), we’re going to introduce a few new functions to show you some other things that R can do and that will also reinforce your understanding of probability. 8.1.1 Simulation One of the most powerful features of R is that you can use it for data simulation. Data simulation is the act of generating random numbers that follow a certain distribution or have known properties. This might not sound particularly impressive, but simulating data means that you can do things such as plan your statistical analyses, understand and demonstrate how probability works, or estimate how many participants you need to test in your experiment based upon what you think the data will look like. Data simulation uses the different types of distributions that we covered in Lab 3 to generate data, so make sure that you’re happy with the probability chapter before you move on. 8.1.2 Activity 1: sample() Just like Lab 3, all the functions we need for simulation are contained in Base R, however, we’ll also load the tidyverse so that we can wrangle our simulated data. Let’s start by introducing the sample() function, which samples elements (data points) from a vector (a collection of things that are of the same type, like numbers or words). We can use sample() to simulate flipping a coin and build some of the graphs you saw in the probability chapter. sample() is used when we want to simulate discrete data, i.e., nominal or ordinal data. sample() requires you to define three arguments: x = a vector of elements, i.e., all of the possible outcomes. For our current example, this would be HEADS and TAILS. size = how many samples do you want to take, i.e., how many times do you want R to flip the coin? replace = specifies whether we should sample with replacement or not. In the last lab we used the example of pulling names out of a hat. If you put the name back in the hat each time you pulled one out this would be with replacement, if you don’t put the name back in this would be sampling without replacement. Basically, do you want to be able to get the same outcome on different samples? For a coin flip, it should be possible to get the same outcome more than once, so we specify TRUE. If we specified FALSE you could only draw as many samples as there were unique values, so in our case we could only flip the coin twice: once it would land on heads, once on tails, and then we would run out of outcomes. Open a new R Markdown document, name it “Pre-class 4” and save it in your Psych 1B folder. Copy, paste, and run the below code in a new code chunk to simulate flipping a coin 4 times (and load the tidyverse). # Notice that because our event labels are strings (text), # we need to enter them into the function in &quot;quotes&quot; library(tidyverse) sample(x = c(&quot;HEADS&quot;, &quot;TAILS&quot;), size = 4, replace = TRUE) ## [1] &quot;HEADS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;HEADS&quot; How many heads did you get? Don’t worry if it’s different to our example. Run the code again. How many heads did you get this time? How many do you get on each turn if you run the code five more times? When you perform data simulation, you’re unlikely to get the same outcome each time you take a sample, just like if you flipped a coin 4 times on 5 separate occasions you would be unlikely to get the same number of heads each time. What’s so useful about data simulation is that we can use the outcomes from lots of different sampling attempts to calculate the probability of a particular outcome, e.g., getting 4 heads from 4 flips of a coin. So that we can add up the number of heads and tails more easily, let’s simulate those coin flips again, but using numerical codes, 1 = HEADS, 0 = TAILS. Now that the outcomes are numeric, we don’t need the combine function c 0:1 means all numbers from 0 to 1 in steps of 1. So basically, 0 and 1. If you wanted to simulate rolling a die, you would write 1:6 which would give you all the whole numbers from 1 to 6. sample(x = 0:1, size = 4, replace = TRUE) ## [1] 1 1 0 1 8.1.3 Activity 2: sum() Now that we’re using ones and zeroes we can count the number of heads by summing the values of the outcomes. The below code will sample our coin flips as above, and then count up the outcomes. Because we’ve coded heads = 1 and tails = 0, we can interpret the sum of all the outcomes as the number of heads. Copy, paste and run the code below in a new code chunk. # This code pipes the output of sample() into sum() which counts up the number of heads/1s. sample(x = 0:1, size = 4, replace = TRUE) %&gt;% sum() ## [1] 1 Run this function multiple times (you can use the green markdown play arrow at the top right of the code chunk to make this easy). In our simulation of five sets of four flips we got 1, 3, 2, 2, and 3 heads. So in only one out of the five simulations did we get exactly one heads, i.e., a proportion of .2 or 20% of the time. 8.1.4 Activity 3: replicate() 1 Let’s repeat the experiment a whole bunch more times. We can have R do this over and over again using the replicate() function. replicate() requires two arguments (although there are other optional arguments if you want to do more complicated tasks): n = the number of times you want to repeat your code expr = the expression, or code, you want to repeat Copy, paste and run the below code into a new code chunk to run the sample function and sum up the outcomes 20 times. replicate(n = 20, expr = sample(0:1, 4, TRUE) %&gt;% sum()) ## [1] 1 1 1 4 1 2 3 2 4 2 3 2 2 0 2 0 2 3 2 3 8.1.5 Monte Carlo simulation Every year, the city of Monte Carlo is the site of innumerable games of chance played in its casinos by people from all over the world. This notoriety is reflected in the use of the term “Monte Carlo simulation” among statisticians to refer to using a computer simulation to estimate statistical properties of a random process. In a Monte Carlo simulation, the random process is repeated over and over again in order to assess its performance over a very large number of trials. It is usually used in situations where mathematical solutions are unknown or hard to compute. Now we are ready to use Monte Carlo simulation to demonstrate the probability of various outcomes. 8.1.6 Activity 4: replicate() 2 We are going to run our coin flip experiment again but this time we are going to run the experiment 50 times (each including 4 coin tosses), and use the same principles to predict the number of heads we will get. Copy, paste, and run the below code to run the simulation and store the result in an object `heads50** using the code below: heads50 &lt;- replicate(50, sample(0:1, 4, TRUE) %&gt;% sum()) heads50 ## [1] 2 1 3 1 2 2 1 1 2 1 3 3 2 3 1 3 2 2 2 0 3 2 3 3 1 4 2 2 2 3 2 1 4 2 1 ## [36] 1 2 3 2 2 3 3 3 1 3 1 3 2 1 2 8.1.7 Activity 5: probability We can estimate the probability of each of the outcomes (0, 1, 2, 3, 4 heads after 4 coin tosses) by counting them up and dividing by the number of experiments. We will do this by putting the results of the replications in a tibble() and then using count(). data50 &lt;- tibble(heads = heads50) %&gt;% # convert to a table group_by(heads) %&gt;% # group by number of possibilities (0,1,2,3,4) summarise(n = n(), # count occurances of each possibility, p=n/50) # &amp; calculate probability (p) of each heads n p 0 1 0.02 1 13 0.26 2 19 0.38 3 15 0.30 4 2 0.04 Your numbers may be slightly different to the ones presented in this book - remember that by default, each time you run a simulation, you will get a different random sample. 8.1.8 Activity 6: visualisation We can then plot a histogram of the outcomes using geom_bar(). # Note: stat = &quot;identity&quot; tells ggplot to use the values of the y-axis variable (p) as the height of the bars in our histogram (as opposed to counting the number of occurances of those values) ggplot(data50, aes(x = heads,y = p)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;purple&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 4 flips (p)&quot;) + theme_minimal() Figure 8.1: No. of heads from 4 coin tosses probability outcomes. Pause here and interpret the above figure What is the estimated probability for flipping 0/4 heads? 1/4 heads? 2/4 heads? 3/4 heads? 4/4 heads? Unfortunately sometimes this calculation will estimate that the probability of an outcome is zero since this outcome never came up when the simulation is run. If you want reliable estimates, you need a bigger sample to minimise the probability that a possible outcome won’t occur. 8.1.9 Activity 7: big data Let’s repeat the Monte Carlo simulation, but with 10,000 trials instead of just 50. All we need to do is change n from 50 to 10000. heads10K &lt;- replicate(n = 10000, expr = sample(0:1, 4, TRUE) %&gt;% sum()) Again, we’ll put the outcome into a table using tibble and calculate counts and probabilities of each outcome using group_by() and summarise(). Remember to try reading your code in full sentences to help you understand what multiple lines of code connected by pipes are doing. How would you read the below code? data10K &lt;- tibble(heads = heads10K) %&gt;% group_by(heads) %&gt;% summarise(n = n(), p=n/10000) Finally, we can visualise this as we did earlier. ggplot(data10K, aes(heads,p)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;purple&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 4 flips (p)&quot;) + theme_minimal() Figure 8.2: 10K coin toss probability outcomes. Using Monte Carlo simulation, we estimate that the probability of getting exactly one head on four throws is about 0.25. The above result represents a probability distribution for all the possible outcomes in our experiments. We can derive lots of useful information from this. For instance, what is the probability of getting two or more heads in four throws? This is easy: the outcomes meeting the criterion are 2, 3, or 4 heads. We can just add these probabilities together like so: data10K %&gt;% filter(heads &gt;= 2) %&gt;% summarise(p2 = sum(p)) ## # A tibble: 1 x 1 ## p2 ## &lt;dbl&gt; ## 1 0.691 You can add probabilities for various outcomes together as long as the outcomes are mutually exclusive, that is, when one outcome occurs, the others can’t occur. For this coin flipping example, this is obviously the case: you can’t simultaneously get exactly two and exactly three heads as the outcome of a single experiment. However, be aware that you can’t simply add probabilities together when the events in question are not mutually exclusive: for example, the probability of the coin landing heads up, and it landing with the image of the head being in a particular orientation are not mutually exclusive, and can’t be simply added together. This is the basis of how we can calculate the probability of an outcome using a known distribution - by simulating a large number of trials we can use this as an estimate for how our data will look in the real world. 8.1.10 Activity 8: rnorm() We can also use R to simulate continuous data that follow a normal distribution using rnorm(). You’ve actually used rnorm() before, all the way back in Lab 2 of Psych 1A but we’ll go over it again. n is the number of data points you wish to simulate which is the only required argument for rnorm() mean is the mean that you want your data to have. If you don’t provide this argument, rnorm() will use a default value of mean = 0. sd is the standard deviation you want your data to have. If you don’t provide this argument, rnorm() will use a default value of sd = 1. Copy, paste and run the below code in a new code chunk. This will randomly generate 50 numbers that collectively have a mean of 10 and a SD of 2 and then store it in the object normal. normal &lt;- rnorm(n = 50, mean = 10, sd = 2) You can check that the data you have generated are as you expected by calculating the mean and SD of this new variable - you shouldn’t expect the values to be exactly 10 and 2 (remember, it’s randomly generated), but they should be reasonably close. mean(normal) sd(normal) ## [1] 9.645085 ## [1] 2.203737 Finally, you can visualise your data with a density plot. Try changing the number of data points generated by rnorm() from 50 to 500 to 5000 and then see how the shape of the distribution changes. tibble(normal = normal) %&gt;% #turn the variable normal into a table and then ggplot(aes(normal)) + # create a density plot geom_density(fill = &quot;red&quot;) + theme_minimal() Figure 4.3: Distribution of variable created by rnorm 8.1.11 Activity 9: Simulate a dataset Finally, we can put all of this together to simulate a full dataset. Let’s imagine that we’re going to run an experiment to see whether 120 people will roll a higher number on a die if their IQ is higher. This is obviously a stupid experiment but psychology does occasionally do stupid things. First, let’s create a variable that has all of our subject IDs. We’re just going to assign our participants numerical codes. subject_id &lt;- 1:120 # create a variable called subject_id that has the numbers 1 to 120 in it Then we’re going to create a column for gender using a new but simple function rep which stands for “repeat”. The below code will create a variable that repeats “man” 40 times, then “women” 40 times, then “non-binary” 40 times. gender &lt;- rep(x = c(&quot;man&quot;, &quot;woman&quot;, &quot;nonbinary&quot;), each = 40) Next, let’s simulate them all rolling a die once using sample(). rolls &lt;- sample(x = 1:6, size = 120, replace = TRUE) Then, let’s simulate their IQ scores. IQ scores are standardised so that overall, the population has an average IQ of 100 and a SD of 15 so we can use this information to simulate the data with rnorm(). iq &lt;- rnorm(n = 120, mean = 100, sd = 15) Finally, we can stitch all these variables together into a table. sim_data &lt;- tibble(subject_id, gender, rolls, iq) Now that we’ve got our simulated data we could write code to analyse it even before we’ve collected any real data which will not only save us time in the future, but will help us plan our analyses and we could include this code in a pre-registration document. For example, you could create a plot of IQ scores for each dice roll (remember these are not real data…) sim_data %&gt;% mutate(rolls = as.factor(rolls)) %&gt;% ggplot(aes(x = rolls, y = iq, fill = rolls)) + geom_violin(trim = FALSE, alpha = .6, show.legend = FALSE) + geom_boxplot(width = .2, show.legend = FALSE) + scale_fill_viridis_d(option = &quot;E&quot;) + theme_minimal() + labs(x = &quot;Outcome of dice roll&quot;) Figure 1.6: Boxplot of IQ scores grouped by what each person rolled on the die 8.1.12 Finished! The in-class activities for Lab 4 are to analyse your group project data which means in terms of new stuff to learn, we’re done! In Psych 1A and 1B, we’ve tried to give you a solid introduction to common data skills you’ll need in order to conduct your own research. Even if you don’t intend to continue with psychology or quantitative research in the future, we hope that you’re proud of the skills you’ve learned. For some of you, R might not have been your favourite bit of the course, but you should be very proud of what you’ve achieved and with such a diverse class we hope you can see that programming isn’t an innate skill that only certain people can learn. It just take a a bit of work, some (hopefully) helpful teaching materials, and a lot of swearing at the error messages. 8.2 In-class activities We’ve spent the last 6 months giving you the skills you need to be able to deal with your own data. Now’s the time to show us what you’ve learned. In this chapter we’re going to describe the steps you will need to go through when analysing your data but, aside from a few lines that will help you deal with the questionnaire data that the Experimentum platform spits out, we’re not going to give you any code solutions. Everything you need to do you’ve done before, so use this book to help you. Remember, you don’t need to write the code from memory, you just need to find the relevant examples and then copy and paste and change what needs changing to make it work for you. We suggest that you problems-solve the code as a group, however, make sure that you all have a separate copy of the final working code. 8.2.1 Step 1: Load in packages and data The data files are on Moodle. Remember, don’t share them in Messenger etc. because of data protection laws. Don’t change ANY of the code from step 1 and 2. Just copy and paste it into R EXACTLY as it is below. library(tidyverse) demo &lt;- read_csv(&quot;demographics.csv&quot;) mslq &lt;- read_csv(&quot;mslq.csv&quot;) teams &lt;- read_csv(&quot;team-name.csv&quot;) 8.2.2 Step 2: Clean up the data Run the below code - don’t change anything. This code will clean up the Experimentum data a little bit to help you on your way. You will get a message saying NAs introduced by coercion. Ignore this message, it’s a result of converting the employment hours to a numeric variable. demo_final &lt;- demo %&gt;% group_by(user_id, q_id) %&gt;% filter(session_id == min(session_id), endtime == min(endtime)) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% filter(user_status %in% c(&quot;guest&quot;, &quot;registered&quot;)) %&gt;% select(user_id, user_sex, user_age, q_name, dv) %&gt;% pivot_wider(names_from = q_name, values_from = dv)%&gt;% mutate(employment = as.numeric(employment)) teams_final &lt;- teams %&gt;% group_by(user_id, q_id) %&gt;% filter(session_id == min(session_id), endtime == min(endtime)) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% filter(user_status %in% c(&quot;guest&quot;, &quot;registered&quot;)) %&gt;% select(user_id, user_sex, user_age, dv) %&gt;% rename(&quot;team&quot; = &quot;dv&quot;) mslq_final &lt;- mslq %&gt;% group_by(user_id, q_id) %&gt;% filter(session_id == min(session_id), endtime == min(endtime)) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% filter(user_status %in% c(&quot;guest&quot;, &quot;registered&quot;)) %&gt;% select(user_id, user_sex, user_age, q_name, dv) %&gt;% arrange(q_name) %&gt;% pivot_wider(names_from = q_name, values_from = dv) Right. Your turn. Remember, coding isn’t about memorising code, it’s about knowing where to look for examples that you can modify with your new variables. You may find it helpful to use the search function in this book. Figure 8.3: Searching for functions 8.2.3 Step 3: Join Join together the data files by their common columns. The resulting dataset is going to have 91 columns which means that R won’t show you them all if you just click on the object, you’ll need to run summary(). Hint: You can only join two objects at once, so you’ll need to do multiple joins (in a pipeline if you’re feeling snazzy). 8.2.4 Step 4. Select your variables Use select to retain only the variables you need for your chosen research design and analysis, i.e. the responses to the sub-scale you’re interested in as well as the user id, sex, age, team name, and any variables you’re going to use as criteria for inclusion. You might find it helpful to consult the MSLQ overview document from the Lab 1 pre-class to get the variable names. 8.2.5 Step 5: Factors Use summary or str to check what type of variable each column is. Recode any necessary variables as factors and then run summary again to see how many you have in each group. You will find the code book you downloaded with the data files from Moodle helpful for this task. You may find the Psych 1A section on factors helpful for this. 8.2.6 Step 6: Filter If necessary, use filter to retain only the observations you need, for example, you might need to delete participants above a certain age, or only use mature students etc. (and make sure you kept all these columns in step 4). Do not filter the data for your team yet. You will find the code book you downloaded with the data files from Moodle helpful for this task. If your grouping variable is whether students undertake paid employment, you will need to create a new variable using mutate that categorises participants into employed (&gt; 0 hours worked per week) and not employed (0 hours per week) categories. An additional bit of syntax you might find useful for this is the %in% notation which allows you to filter by multiple values. For example, the following code will retain all rows where user_sex equals male OR female and nothing else (i.e., it would get rid of non-binary participants, prefer not to says, and missing values). dat %&gt;% filter(user_sex %in% c(&quot;male&quot;, &quot;female&quot;)) You can also do it by exclusion with !. The below code would retain everything where user_sex doesn’t equal male or female. dat %&gt;% filter(!user_sex %in% c(&quot;male&quot;, &quot;female&quot;)) If you were feeling really fancy you could do steps 3 - 6 in a single pipeline of code. 8.2.7 Step 7: Sub-scale scores Calculate the mean score for each participant for your chosen sub-scale. There are a few ways you can do this but helpfully the Experimentum documentation provides example code to make this easier, you just need to adapt it for the variables you need. You may also want to change the na.rm = TRUE for the calculation of means depending on whether you want to only include participants who completed all questions. dat_means &lt;- data %&gt;% # change data to the name of the data object you want to work from pivot_longer(names_to = &quot;var&quot;, values_to = &quot;val&quot;, question_1:question_5) %&gt;% # change question_1:question_5 to the relevant variables for your sub-scale, don&#39;t change anything else group_by_at(vars(-val, -var)) %&gt;% # don&#39;t change this at all summarise(scale_mean = mean(val, na.rm = TRUE)) %&gt;% # change scale_mean to the name of your sub-scale, e.g., anxiety_mean ungroup() # don&#39;t change this at all 8.2.8 Step 8: Split the dataset Next, use filter again to create a new dataset that only contains the data from participants who contributed to your team and call it dat_means_team. Once this is complete, you will have the final large dataset that contains the scale scores for all participants, and a smaller dataset that just has data from the participants you recruited. Use the codebook to find which number corresponds to your team. 8.2.9 Step 9: Demoraphic information That should be the really hard bit done, now you’ve got the data in the right format for analysis. First, calculate the demographic information you need: number of participants, gender split, grouping variable split (if you’re using a variable that’s not gender), mean age and SD. You can calculate mean age and SD using summarise() like you’ve done before. There’s several different ways that you can count the number of participants in each group, we haven’t explicitly shown you how to do this yet so we’ll give you example code for this below. The code is fairly simple, you just need to plug in the variables you need. Do this separately for the full dataset and your team dataset. # count the total number of participants in the dataset dat_means %&gt;% count() # count the number of responses to each level of user_sex (for gender) dat_means %&gt;% group_by(user_sex) %&gt;% count() # count the number of responses to each level of mature student status (you may need to change this variable to the one you&#39;re using) dat_means %&gt;% group_by(mature) %&gt;% count() # count the number of responses across two categories (you might not need or want to do this) dat_means %&gt;% group_by(user_sex,mature) %&gt;% count() Once you’ve done this you might realise that you have participants in the dataset that shouldn’t be there. For example, you might have people who have answered “Not applicable” to the mature student question, or you might have some NAs (missing data from when people didn’t respond). You need to think about whether you need to get rid of any observationse from your dataset. For example, if you’re looking at gender differences, then you can’t have people who are missing gender information. You may have said in your pre-reg that you would only include non-binary people if they made up a certain proportion of the data. If you’re looking at mature student status, you can’t have people who didn’t answer the question or who said not applicable (i.e., postgrad students). You need to decide whether any of this is a problem, and potentially go back and add in an extra filter to step 6. 8.2.10 Step 10: Descriptive statistics Use summarise and group_by to calculate the mean, median, and standard deviation of the sub-scale scores for each group. Do this separately for the full dataset and your team dataset. 8.2.11 Step 11: visualisation You now need to create a bar chart with error bars, a violin-boxplot and a grouped density plot for both the full dataset and your team dataset. You’ve done all of these before, just find a previous example code and change the variables and axis labels. It’s up to you how your present these in presentation. You might want to put all of the team graphs on one slide and then all of the overall dataset graphs on another. Or you might decide that it is better to put the same type of graphs on the same slide, to help comparisons. We want you to think about what will communicate the data best, so it’s up to you. ` "],
["cloud.html", "A RStudio Cloud A.1 Creating an Account A.2 Accessing RStudio Cloud A.3 Getting Started with RStudio Cloud", " A RStudio Cloud Sometimes R might not work well on our own computers. However, there is an online version of R Studio (R Studio Cloud) which can be used in a pinch. Using R Studio Cloud is a little different to R Studio, so we have made a short guide to get you up and running. Please take a minute to read the GDPR guidance for using RStudio Cloud. A.1 Creating an Account Head to RStudio Cloud and click “Sign Up” at the top of the page Enter details you wish to sign in with and select “Sign up” You’ll receive an e-mail at the address you sign up with, make sure to click the link to activate your account fully. A.2 Accessing RStudio Cloud Head back to RStudio Cloud and select “log In”, where we previously chose “Sign Up” Once logged in, you’ll be taken to the “Your Workspace” page, this is where all of your RStudio Cloud projects will be accessible from Select “New Project” and then “New Project” again You will see the message “Deploying Project” for a couple of minutes while it creates your Workspace A.3 Getting Started with RStudio Cloud Once loaded, you’ll see a page that looks almost identical to the other screenshots in the learning material A.3.1 Naming the Workspace Let’s give the project a better name! Click on “Untitled Project” at the top of the page This will allow you to rename to whatever you like, in this case we’ll go for “Network Training” Press the return key on your keyboard, or click on a different area on the page to complete the task A.3.2 Uploading Files Since this is on the web, files on your computer won’t be immediately accessible to RStudio Cloud, you will need to upload them yourself Click the “Upload” button on the Files tab An “Upload Files” element will load up where you can click “Browse” and select the file(s) you wish to make available to RStudio Cloud Once you have selected a file and chosen “OK”, you’ll be taken back to the main application and you will now see the file you uploaded You can now interact with this file as described in the rest of the learning material! "],
["homework-instructions-1.html", "B Homework instructions", " B Homework instructions Just like you’ve done throughout this book so far, we’re going to use R Markdown for the homework worksheets. There are just a couple of important rules we need you to follow to make sure this all runs smoothly. These worksheets will ask you to fill in your answers and not change any other information. For example, if we ask you to replace NULL with your answer, only write in the code you are giving as your answer and nothing else. To illustrate - Task 1 read in your data data &lt;- NULL The task above is to read in the data file we are using for this task - the correct answer is data &lt;- read_csv(data.csv). You would replace the NULL with: Solution to Task 1 data &lt;- read_csv(&quot;data.csv&quot;) This means that we can look for your code and if it is in the format we expect to see it in, we can give you the marks! If you decide to get all creative on us then we can’t give you the marks as ‘my_work_Nov_2018.csv’ isn’t the filename we have given to you to use. So don’t change the file, variable or data frame names as we need these to be consistent. We will look for your answers within the boxes which start and end with ``` and have {r task name} in them e.g. ```{r tidyverse, messages=FALSE} library(tidyverse) ``` These are called code chunks and are the part of the worksheet that we can read and pick out your answers. If you change these in any way we can’t read your answer and therefore we can’t give you marks. You can see in the example above that the code chunk (the grey zone), starts and ends with these back ticks (usually found on top left corner of the keyboard). This code chunk has the ticks and text which makes it the part of the worksheet that will contain code. The {r tidyverse} part tells us which task it is (e.g., loading in tidyverse) and therefore what we should be looking for and what we can give marks for - loading in the package called tidyverse in the example above. If this changes then it won’t be read properly, so will impact on your grade. The easiest way to use our worksheets is to think of them as fill-in-the-blanks and keep the file names and names used in the worksheet the same. If you are unsure about anything then use the forums on Moodle and Teams to ask any questions. "]
]
